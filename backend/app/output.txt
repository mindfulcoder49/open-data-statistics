./__init__.py
./middleware.py
# app/middleware.py
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response
from fastapi import HTTPException

class UserHeaderMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        user_id = request.headers.get("X-User-Name")
        if not user_id:
            raise HTTPException(status_code=400, detail="X-User-Name header is required")

        # Optionally, you can add more user-related logic here, such as authentication validation

        response = await call_next(request)
        return response
./utils/__init__.py
./utils/helpers.py
import json
import os
import time
import logging

from bs4 import BeautifulSoup

import subprocess

from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from asyncio import Semaphore

logger = logging.getLogger(__name__)

semaphore = Semaphore(5) # Limit the number of concurrent Playwright instances

def strip_irrelevant_tags_and_attributes(html_content):
    """Strip out non-essential tags and attributes from HTML content to simplify it."""
    try:
        soup = BeautifulSoup(html_content, "html.parser")

        # Define tags to be removed
        tags_to_remove = [
            "script", "style", "svg", "img", "iframe", "link", "noscript",
            "meta", "base", "embed", "object", "param", "source"
        ]

        # Remove all specified tags from the soup
        for tag in tags_to_remove:
            for element in soup.find_all(tag):
                element.decompose()  # Remove the element from the tree

        # Define attributes to be removed from all tags
        attributes_to_remove = ["style", "onclick", "onmouseover", "onload"]  # Add more if needed

        # Iterate over all tags and remove the specified attributes
        for element in soup.find_all(True):  # True matches all tags
            for attribute in attributes_to_remove:
                if attribute in element.attrs:
                    del element.attrs[attribute]

        # Return the cleaned HTML content
        return str(soup)
    except Exception as e:
        print(f"Error stripping irrelevant tags and attributes: {e}")
        return ""

async def fetch_html_with_playwright(url, wait=3000, navigation_timeout=60000):
    async with semaphore:
        """Fetch the HTML content of a given URL using Playwright and simplify it."""
        browser = None
        page = None
        try:
            # Initialize Playwright
            async with async_playwright() as p:
                # Launch the browser in headless mode
                browser = await p.chromium.launch(headless=True)
                logger.info(f"Waiting for {wait / 1000} seconds for the page to load...")

                # Create a new page
                page = await browser.new_page()

                # Set navigation timeout
                page.set_default_navigation_timeout(navigation_timeout)  # Adjust navigation timeout
                page.set_default_timeout(navigation_timeout)  # Adjust all action timeouts

                # Navigate to the URL
                await page.goto(url, timeout=navigation_timeout, wait_until="domcontentloaded")


                # Wait for additional content load
                await page.wait_for_timeout(wait)

                # Get the page content
                html_content = await page.content()
                return strip_irrelevant_tags_and_attributes(html_content)
        except Exception as e:
            logger.error(f"Error fetching {url} with Playwright: {e}")
            return ""
        finally:
            # Ensure browser closes even if an error occurs
            if page:
                await page.close()
            if browser:
                await browser.close()
./main.py
# main.py
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from app.middleware import UserHeaderMiddleware
from app.routes import scrape
import os
from dotenv import load_dotenv
import logging

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,  # Set the desired logging level
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",  # Log format
)
logger = logging.getLogger(__name__)

# Load environment variables from the .env file
load_dotenv()

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add middleware to extract user information from the request headers
app.add_middleware(UserHeaderMiddleware)

# Include your routers
app.include_router(scrape.router)

# Mount the static files for the frontend
#app.mount("/", StaticFiles(directory="../frontend/build", html=True), name="static")




./routes/__init__.py
./routes/scrape.py

from fastapi import APIRouter, HTTPException, Depends, Query, Path, Header  
from sqlalchemy.orm import Session
from typing import List, Optional, Any
from pydantic import BaseModel
from app.utils.helpers import fetch_html_with_playwright, strip_irrelevant_tags_and_attributes
import aiofiles
import logging
from fastapi.responses import StreamingResponse
import asyncio
import json

# Set up a logger
logger = logging.getLogger(__name__)

router = APIRouter()

# Models for request validation
class ScrapeUrlRequest(BaseModel):
    url: str
    wait: Optional[int] = 3


@router.post("/scrape_url", response_model=str)
async def scrape_base_url(request: ScrapeUrlRequest):
    """Scrape event URLs from the given base URL."""
    logger.info(f"Scraping event URLs from base URL: {request.url}")
    
    # Await the asynchronous fetch_html_with_playwright function
    html_content = await fetch_html_with_playwright(request.url, request.wait * 1000)
    
    if not html_content:
        logger.warning("Failed to fetch HTML content")
        raise HTTPException(status_code=404, detail="Failed to fetch HTML content")
    
    logger.info(f"Fetched HTML content from {request.url}")
    return html_content
./models/__init__.py
