diff --git a/backend/app/__pycache__/main.cpython-312.pyc b/backend/app/__pycache__/main.cpython-312.pyc
index 5c5ce60..535377f 100644
Binary files a/backend/app/__pycache__/main.cpython-312.pyc and b/backend/app/__pycache__/main.cpython-312.pyc differ
diff --git a/backend/app/main.py b/backend/app/main.py
index bb6e451..c724b03 100644
--- a/backend/app/main.py
+++ b/backend/app/main.py
@@ -16,13 +16,15 @@ import numpy as np
 from pydantic_settings import BaseSettings, SettingsConfigDict
 from typing import List, Dict, Any, Optional, Literal
 from celery import Celery
+from kombu import Queue
 import redis
+import requests
+from app.config import settings
+from core.storage import JsonStorageModel, ImageStorageModel
 
 from stages.stage2_yearly_count_comparison import Stage2YearlyCountComparison
 from stages.stage3_univariate_anomaly import Stage3UnivariateAnomaly
 from stages.stage4_h3_anomaly import Stage4H3Anomaly
-from reporting.stage3_reporter import Stage3Reporter
-from reporting.stage4_reporter import Stage4Reporter
 
 # Configure logging
 logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
@@ -31,19 +33,6 @@ logger = logging.getLogger(__name__)
 # Load environment variables
 load_dotenv()
 
-# --- Contents from config.py ---
-class Settings(BaseSettings):
-    REDIS_URL: str = Field("redis://redis:6379/0", env="REDIS_URL")
-    # Local path inside the container for storing results
-    RESULTS_DIR: str = "/app/results"
-    
-    # The hostname that internal services (like Celery workers) should use
-    # to communicate with the API. In Docker Compose, this is the service name.
-    INTERNAL_API_HOSTNAME: str = "http://backend:8080"
-
-    model_config = SettingsConfigDict(env_file=".env", env_file_encoding='utf-8', extra='ignore')
-
-settings = Settings()
 
 # Ensure the results directory exists
 os.makedirs(settings.RESULTS_DIR, exist_ok=True)
@@ -103,10 +92,6 @@ AVAILABLE_STAGES = {
     "stage3_univariate_anomaly": Stage3UnivariateAnomaly,
     "stage4_h3_anomaly": Stage4H3Anomaly,
 }
-AVAILABLE_REPORTERS = {
-    "stage3_univariate_anomaly": Stage3Reporter,
-    "stage4_h3_anomaly": Stage4Reporter,
-}
 
 
 # --- Contents from core/pipeline_manager.py ---
@@ -233,17 +218,31 @@ class UniqueValuesRequest(BaseModel):
     file_path: str
     column_name: str
 
+class CompletionRequest(BaseModel):
+    job_id: str
+    prompt: str
+    model: str = "llama3"
 
 # --- Contents from tasks.py ---
 # Initialize Celery
-celery_app = Celery('tasks', broker=settings.REDIS_URL, backend=settings.REDIS_URL)
+# Use CELERY_BROKER_URL if provided, otherwise fall back to REDIS_URL.
+# This allows the worker to connect from a different network.
+broker_url = settings.CELERY_BROKER_URL or settings.REDIS_URL
+celery_app = Celery('tasks', broker=broker_url, backend=broker_url)
+
+# Define queues
+celery_app.conf.task_queues = (
+    Queue('celery'),  # Default queue for analysis tasks
+    Queue('completions'),  # Dedicated queue for AI completions
+)
+celery_app.conf.task_default_queue = 'celery'
+
 celery_app.conf.update(
     task_track_started=True,
 )
 
 # Initialize a Redis client for custom status updates
 redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)
-
 @celery_app.task(bind=True)
 def run_analysis_pipeline(self, job_id: str, data_sources: list, config: dict):
     """
@@ -272,6 +271,76 @@ def run_analysis_pipeline(self, job_id: str, data_sources: list, config: dict):
         # Re-raise the exception so Celery knows the task failed
         raise
 
+@celery_app.task(bind=True, name="tasks.process_completion_request")
+def process_completion_request(self, job_id: str, prompt: str, model: str):
+    """
+    Celery task to process a completion request using a local Ollama service.
+    """
+    json_storage = JsonStorageModel()
+    try:
+        status = {"status": "processing", "stage": "starting"}
+        redis_client.set(f"job_status:{job_id}", json.dumps(status))
+
+        logger.info(f"[{job_id}] Sending prompt to Ollama model {model} at {settings.OLLAMA_URL}")
+        
+        ollama_payload = {
+            "model": model,
+            "prompt": prompt,
+            "stream": False  # We want the full response at once
+        }
+        # Add a timeout to the request (e.g., 5 minutes)
+        response = requests.post(f"{settings.OLLAMA_URL}/api/generate", json=ollama_payload, timeout=300)
+        response.raise_for_status()
+        
+        ollama_result = response.json()
+        
+        final_status = {
+            "status": "completed",
+            "response": ollama_result.get("response"),
+            "context": ollama_result.get("context")
+        }
+        
+        # Save the result to a JSON file
+        json_storage.save(job_id, "completion.json", final_status)
+        
+        redis_client.set(f"job_status:{job_id}", json.dumps({"status": "completed"}))
+        logger.info(f"[{job_id}] Ollama completion successful and saved.")
+        return final_status
+
+    except requests.exceptions.HTTPError as e:
+        logger.error(f"[{job_id}] HTTP error connecting to Ollama: {e}", exc_info=True)
+        error_message = f"Ollama service returned an error: {e}. Check if the Ollama server is running and the model '{model}' is available."
+        if e.response.status_code == 404:
+            error_message += " A 404 error suggests the Ollama API endpoint was not found. Check your OLLAMA_URL and any proxy settings."
+        error_status = {"status": "failed", "error_message": error_message}
+        redis_client.set(f"job_status:{job_id}", json.dumps(error_status))
+        raise
+    except requests.exceptions.RequestException as e:
+        logger.error(f"[{job_id}] Failed to connect to Ollama: {e}", exc_info=True)
+        error_status = {"status": "failed", "error_message": f"Could not connect to Ollama service at {settings.OLLAMA_URL}. Is it running and accessible? Error: {e}"}
+        redis_client.set(f"job_status:{job_id}", json.dumps(error_status))
+        raise
+    except Exception as e:
+        logger.error(f"[{job_id}] Completion task failed: {e}", exc_info=True)
+        error_status = {"status": "failed", "error_message": str(e)}
+        redis_client.set(f"job_status:{job_id}", json.dumps(error_status))
+        raise
+
+@celery_app.task(name="tasks.get_available_models")
+def get_available_models():
+    """
+    Celery task to be executed by the completions worker to fetch models from Ollama.
+    """
+    try:
+        logger.info(f"Task 'get_available_models' fetching from {settings.OLLAMA_URL}")
+        response = requests.get(f"{settings.OLLAMA_URL}/api/tags", timeout=10)
+        response.raise_for_status()
+        return response.json()
+    except requests.exceptions.RequestException as e:
+        logger.error(f"Task 'get_available_models' failed to connect to Ollama: {e}")
+        # Celery will propagate this exception to the caller waiting on the result.
+        raise
+
 # --- API Endpoints ---
 
 @app.get("/api/v1/data/list")
@@ -367,6 +436,53 @@ async def upload_data_file(request: Request, file: UploadFile = File(...)):
     
     return {"file_path": relative_path}
 
+@app.post("/api/v1/completions", response_model=JobCreateResponse, status_code=202)
+async def create_completion_job(request_data: CompletionRequest, request: Request):
+    """
+    Accepts a completion job, validates it, and queues it for the 'completions' worker.
+    """
+    job_id = request_data.job_id
+    logger.info(f"Received completion job request: {job_id}")
+
+    # Dispatch the task to the 'completions' queue
+    task = process_completion_request.apply_async(
+        args=[job_id, request_data.prompt, request_data.model],
+        queue='completions'
+    )
+
+    # Store initial status in Redis
+    initial_status = {
+        "status": "queued",
+        "task_id": task.id,
+        "request_payload": request_data.dict()
+    }
+    await app.state.redis.set(f"job_status:{job_id}", json.dumps(initial_status))
+
+    base_url = str(request.base_url)
+    return JobCreateResponse(
+        job_id=job_id,
+        status_url=f"{base_url}api/v1/jobs/{job_id}/status",
+        results_url=f"{base_url}api/v1/jobs/{job_id}/results"
+    )
+
+@app.get("/api/v1/completions/models")
+async def get_ollama_models():
+    """
+    Fetches the list of available models by dispatching a task to the completions worker.
+    """
+    try:
+        # This is a blocking call. We wait for the worker to respond.
+        # A timeout is crucial to prevent the API from hanging if the worker is down.
+        async_result = get_available_models.apply_async(queue='completions')
+        result = async_result.get(timeout=15) 
+        return result
+    except Exception as e:
+        logger.error(f"Could not get models from completions worker: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=502, 
+            detail="Could not retrieve models from the completions worker. It may be offline or unable to connect to Ollama."
+        )
+
 # --- Admin Endpoints ---
 
 @app.get("/api/v1/admin/jobs")
@@ -542,11 +658,9 @@ async def get_job_results_list(job_id: str, request: Request):
     if status.get("status") != "completed":
         raise HTTPException(status_code=400, detail=f"Job is not complete. Current status: {status.get('status')}")
 
-    job_dir = os.path.join(settings.RESULTS_DIR, job_id)
-    if not os.path.isdir(job_dir):
-        return {"job_id": job_id, "results": {}}
-
-    files = os.listdir(job_dir)
+    json_storage = JsonStorageModel()
+    files = json_storage.list_artifacts(job_id)
+    
     base_url = str(request.base_url)
     results_urls = {}
     
@@ -600,19 +714,19 @@ async def get_job_result_artifact(job_id: str, artifact_name: str):
     """
     Retrieves a specific result artifact file for a job.
     """
-    artifact_path = os.path.join(settings.RESULTS_DIR, job_id, artifact_name)
+    if artifact_name.lower().endswith('.png'):
+        storage = ImageStorageModel()
+    else:
+        storage = JsonStorageModel()
 
-    if not os.path.exists(artifact_path):
+    if not storage.exists(job_id, artifact_name):
         raise HTTPException(status_code=404, detail="Artifact not found")
 
-    # Determine content type for images
-    media_type = None
-    if artifact_name.lower().endswith('.png'):
-        media_type = 'image/png'
-    elif artifact_name.lower().endswith('.json'):
-        media_type = 'application/json'
+    response = storage.get_response(job_id, artifact_name)
+    if not response:
+        raise HTTPException(status_code=500, detail="Could not retrieve file.")
     
-    return FileResponse(artifact_path, media_type=media_type)
+    return response
 
 # --- Frontend Serving Endpoints ---
 
@@ -647,3 +761,11 @@ async def serve_stage4_viewer():
     if not os.path.exists(viewer_path):
         raise HTTPException(status_code=404, detail="Stage 4 viewer not found.")
     return FileResponse(viewer_path)
+
+@app.get("/completions", response_class=FileResponse)
+async def serve_completions_page():
+    """Serves the completions chat interface."""
+    viewer_path = os.path.join(VIEWERS_DIR, "completions.html")
+    if not os.path.exists(viewer_path):
+        raise HTTPException(status_code=404, detail="completions.html not found.")
+    return FileResponse(viewer_path)
diff --git a/backend/app/worker.py b/backend/app/worker.py
index c1f80a1..3cc6bf7 100644
--- a/backend/app/worker.py
+++ b/backend/app/worker.py
@@ -3,5 +3,4 @@ from app.main import celery_app as app
 # This file is the entry point for the Celery worker.
 # The 'app' variable must be defined for Celery to find the application instance.
 # To run the worker, you would use the command:
-
 # celery -A app.worker:app worker --loglevel=info --concurrency=1
diff --git a/backend/reporting/__init__.py b/backend/reporting/__init__.py
deleted file mode 100644
index 679a5bb..0000000
--- a/backend/reporting/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-# This file is intentionally left blank. The contents have been moved to app/main.py.
diff --git a/backend/reporting/stage3_reporter.py b/backend/reporting/stage3_reporter.py
deleted file mode 100644
index fbbecd7..0000000
--- a/backend/reporting/stage3_reporter.py
+++ /dev/null
@@ -1,292 +0,0 @@
-import pandas as pd
-import os
-from .visualizations.common import plot_comparative_time_series, plot_raw_and_aggregated_data
-from .visualizations.stage3 import plot_anomaly_time_series, plot_trend_time_series
-from scipy import stats
-import numpy as np
-from typing import Optional
-
-class Stage3Reporter:
-    """
-    Generates a scholarly HTML report from the Stage 3 Univariate Anomaly results,
-    including data visualizations and robust statistical explanations.
-    """
-
-    @property
-    def file_extension(self) -> str:
-        return "html"
-
-    def generate_report(self, data: dict, df: Optional[pd.DataFrame] = None) -> str:
-        params = data.get('parameters', {})
-        primary_col_name = params.get('primary_group_col', 'Primary Group')
-        secondary_col_name = params.get('secondary_group_col', 'Secondary Group')
-        timestamp_col_name = data.get('parameters', {}).get('timestamp_col', self._find_timestamp_col(df))
-        results = data.get('results', [])
-        city_wide_results = data.get('city_wide_results', [])
-        job_dir = os.path.dirname(data.get('__filepath__', '.'))
-
-        if not results and not city_wide_results:
-            return "<h1>Scholarly Report on Univariate Time Series Analysis</h1><p>No results were generated.</p>"
-
-        # Combine localized and city-wide results for unified processing
-        for r in results:
-            r['primary_group_name'] = r[primary_col_name]
-        
-        report_df = pd.DataFrame(results + city_wide_results)
-        
-        p_value_threshold = 0.05
-        
-        report_lines = self._generate_header_and_methodology(primary_col_name, secondary_col_name, p_value_threshold)
-
-        report_lines.append("<h2>2. Analysis by Group</h2>")
-
-        if df is not None and timestamp_col_name:
-            report_lines.append("<h3>2.1 Overall Data Overview</h3>")
-            report_lines.append("<p>The following plot shows the weekly aggregated counts and distributions by day and hour for the entire dataset. This provides a high-level context for the detailed findings below.</p>")
-            initial_plot_filename = plot_raw_and_aggregated_data(df, timestamp_col_name, job_dir)
-            report_lines.append(f'<img src="{initial_plot_filename}" alt="Initial Data Aggregation" style="width:100%; max-width:800px;">')
-            report_lines.append("<h3>2.2 Summary of Significant Findings</h3>")
-        else:
-            report_lines.append("<h3>2.1 Summary of Significant Findings</h3>")
-
-        all_findings = []
-        for _, row in report_df.iterrows():
-            trend_p = row['trend_analysis']['p_value']
-            if trend_p is not None and trend_p < p_value_threshold:
-                all_findings.append({'type': 'Trend', 'group': row['primary_group_name'], 'details': row})
-            
-            for week in row['last_4_weeks_analysis']:
-                anomaly_p = week['anomaly_p_value']
-                if anomaly_p is not None and anomaly_p < p_value_threshold:
-                    if row['historical_weekly_avg'] < 1 and week['count'] == 1:
-                        continue
-                    finding = {'type': 'Anomaly', 'group': row['primary_group_name'], 'details': row.to_dict(), 'week_details': week}
-                    all_findings.append(finding)
-
-        if not all_findings:
-            report_lines.append("<p>No statistically significant trends or anomalies were detected.</p>")
-        else:
-            city_wide_map = {item[secondary_col_name]: item for item in city_wide_results}
-            
-            # Sort all findings by primary group, then by p-value
-            sorted_findings = sorted(all_findings, key=lambda x: (
-                x['group'], 
-                x['details']['trend_analysis']['p_value'] if x['type'] == 'Trend' else x['week_details']['anomaly_p_value']
-            ))
-
-            # --- Generate Summary Table ---
-            report_lines.append('<table><thead><tr>'
-                                f'<th>{primary_col_name}</th><th>{secondary_col_name}</th><th>Finding Type</th><th>Date/Period</th>'
-                                '<th>Details</th><th>P-Value</th><th>Z-Score / Slope</th>'
-                                '</tr></thead><tbody>')
-            for finding in sorted_findings:
-                details = finding['details']
-                row_html = f"<tr><td>{details['primary_group_name']}</td><td>{details[secondary_col_name]}</td>"
-                if finding['type'] == 'Trend':
-                    trend = details['trend_analysis']
-                    p_val = trend['p_value']
-                    row_html += f"<td>Trend</td><td>Last 4 Weeks</td><td>{trend['description']}</td><td>{p_val:.4g}</td><td>{trend['slope']:.2f}</td>"
-                else: # Anomaly
-                    week = finding['week_details']
-                    p_val = week['anomaly_p_value']
-                    row_html += f"<td>Anomaly</td><td>{week['week']}</td><td>Count: {week['count']} (vs avg {details['historical_weekly_avg']:.2f})</td><td>{p_val:.4g}</td><td>{week['z_score']:.2f}</td>"
-                row_html += "</tr>"
-                report_lines.append(row_html)
-            report_lines.append('</tbody></table>')
-
-            # --- Detailed Breakdown, Grouped by Primary Column ---
-            if df is not None and timestamp_col_name:
-                report_lines.append("<h3>2.3 Detailed Analysis of Findings</h3>")
-            else:
-                report_lines.append("<h3>2.2 Detailed Analysis of Findings</h3>")
-
-            current_group = None
-            for i, finding in enumerate(sorted_findings):
-                details = finding['details']
-                group_name = details['primary_group_name']
-                
-                if group_name != current_group:
-                    if current_group is not None: report_lines.append("</div>") # Close previous group div
-                    report_lines.append(f'<div class="finding-group"><h4>Group: {group_name}</h4>')
-                    current_group = group_name
-
-                sec_group = details[secondary_col_name]
-                city_wide_data = city_wide_map.get(sec_group)
-                
-                finding_id = f"finding-{i+1}"
-                report_lines.append(f'<div class="finding-card" id="{finding_id}">')
-
-                if finding['type'] == 'Trend':
-                    trend_details = details['trend_analysis']
-                    p_val = trend_details['p_value']
-                    report_lines.append(f"<h5>Finding {i+1}: Trend in '{sec_group}' for {details['primary_group_name']}</h5><ul>")
-                    report_lines.append(f"<li><strong>Description</strong>: {trend_details['description']}</li>")
-                    report_lines.append(f"<li><strong>Weekly Change (Slope)</strong>: {trend_details['slope']:.2f}</li>")
-                    report_lines.append(f"<li><strong>Significance (p-value)</strong>: {p_val:.4g}</li>")
-                    
-                    if city_wide_data and details['primary_group_name'] != 'City-Wide':
-                        cw_trend = city_wide_data['trend_analysis']
-                        report_lines.append(f"<li><strong>City-Wide Context</strong>: The trend for '{sec_group}' across all groups is: <strong>{cw_trend['description']}</strong> (p-value: {cw_trend['p_value']:.4g}, slope: {cw_trend['slope']:.2f}).</li>")
-                    report_lines.append("</ul>")
-                    plot_filename = self._generate_comparative_plot(details, city_wide_data, primary_col_name, secondary_col_name, job_dir)
-                    report_lines.append(f'<img src="{plot_filename}" alt="Time series for {sec_group}" style="width:100%; max-width:600px;">')
-
-                else: # Anomaly
-                    week_details = finding['week_details']
-                    p_val = week_details['anomaly_p_value']
-                    report_lines.append(f"<h5>Finding {i+1}: Anomaly in '{sec_group}' for {details['primary_group_name']}</h5><ul>")
-                    report_lines.append(f"<li><strong>Date</strong>: {week_details['week']}</li>")
-                    report_lines.append(f"<li><strong>Observed Count</strong>: {week_details['count']} (Historical Avg: {details['historical_weekly_avg']:.2f})</li>")
-                    report_lines.append(f"<li><strong>Magnitude (Z-Score)</strong>: {week_details['z_score']:.2f}</li>")
-                    report_lines.append(f"<li><strong>Significance (p-value)</strong>: {p_val:.4g}</li>")
-                    
-                    if city_wide_data and details['primary_group_name'] != 'City-Wide':
-                        cw_week_data = next((w for w in city_wide_data['last_4_weeks_analysis'] if w['week'] == week_details['week']), None)
-                        if cw_week_data:
-                            cw_p_val = cw_week_data['anomaly_p_value']
-                            status = "significant" if cw_p_val < p_value_threshold else "not significant"
-                            report_lines.append(f"<li><strong>City-Wide Context</strong>: The same week was <strong>{status}</strong> for '{sec_group}' across all groups (p-value: {cw_p_val:.4g}).</li>")
-                    report_lines.append("</ul>")
-                    plot_filename = self._generate_comparative_plot(details, city_wide_data, primary_col_name, secondary_col_name, job_dir, anomaly_points=[week_details])
-                    report_lines.append(f'<img src="{plot_filename}" alt="Time series for {sec_group}" style="width:100%; max-width:600px;">')
-                
-                report_lines.append('</div>') # Close finding-card
-            
-            if current_group is not None: report_lines.append("</div>") # Close final group div
-
-        report_lines.extend(self._generate_appendix())
-        return "\n".join(report_lines)
-
-    def _find_timestamp_col(self, df: Optional[pd.DataFrame]) -> Optional[str]:
-        """Helper to find a timestamp column if not explicitly provided."""
-        if df is None:
-            return None
-        for col in df.columns:
-            if pd.api.types.is_datetime64_any_dtype(df[col]):
-                return col
-        return None
-
-    def _generate_comparative_plot(self, group_data: dict, city_wide_data: Optional[dict], primary_col: str, secondary_col: str, job_dir: str, anomaly_points: Optional[list] = None) -> str:
-        """Helper to generate a comparative plot for a given finding."""
-        group_series = pd.Series(group_data['full_weekly_series'])
-        group_series.index = pd.to_datetime(group_series.index)
-
-        city_wide_series = None
-        if city_wide_data and group_data['primary_group_name'] != 'City-Wide':
-            city_wide_series = pd.Series(city_wide_data['full_weekly_series'])
-            city_wide_series.index = pd.to_datetime(city_wide_series.index)
-
-        return plot_comparative_time_series(
-            group_series=group_series,
-            group_name=group_data['primary_group_name'],
-            city_wide_series=city_wide_series,
-            primary_col=primary_col,
-            secondary_col=group_data[secondary_col],
-            output_dir=job_dir,
-            anomaly_points=anomaly_points
-        )
-
-    def _generate_plot_for_trend(self, trend_data: dict, primary_col: str, secondary_col: str, job_dir: str) -> str:
-        """Helper to generate a plot for a given trend and return the filename."""
-        # Reconstruct the weekly counts for plotting from the full series
-        weekly_counts = pd.Series(trend_data['full_weekly_series'])
-        weekly_counts.index = pd.to_datetime(weekly_counts.index)
-
-        return plot_trend_time_series(
-            weekly_counts=weekly_counts,
-            trend_analysis=trend_data['trend_analysis'],
-            primary_col=f"{primary_col}: {trend_data[primary_col]}",
-            secondary_col=trend_data[secondary_col],
-            output_dir=job_dir
-        )
-
-    def _generate_plot_for_anomaly(self, anomaly_data: dict, primary_col: str, secondary_col: str, job_dir: str) -> str:
-        """Helper to generate a plot for a given anomaly and return the filename."""
-        row = anomaly_data['details']
-        
-        # Reconstruct the distribution for plotting
-        if row['model_used'].startswith('Poisson'):
-            dist = stats.poisson(mu=row['historical_weekly_avg'])
-        else:
-            var = row['historical_weekly_var']
-            avg = row['historical_weekly_avg']
-            # Basic safety checks for NB parameter calculation
-            if var > avg and avg > 0:
-                p = avg / var
-                n = avg * p / (1 - p)
-                if not (np.isfinite(p) and np.isfinite(n) and 0 < p <= 1 and n > 0):
-                     dist = stats.poisson(mu=avg) # Fallback
-                else:
-                     dist = stats.nbinom(n=n, p=p)
-            else:
-                dist = stats.poisson(mu=avg) # Fallback
-
-        # Reconstruct the weekly counts for plotting
-        weekly_counts = pd.Series(row['full_weekly_series'])
-        weekly_counts.index = pd.to_datetime(weekly_counts.index)
-
-        return plot_anomaly_time_series(
-            weekly_counts=weekly_counts,
-            historical_dist=dist,
-            anomaly_points=[anomaly_data['week_details']],
-            primary_col=f"{primary_col}: {row[primary_col]}",
-            secondary_col=row[secondary_col],
-            output_dir=job_dir
-        )
-
-    def _generate_header_and_methodology(self, primary_col, secondary_col, p_thresh):
-        # Basic CSS for better readability
-        style = """
-<style>
-    body { font-family: sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px; }
-    h1, h2, h3, h4, h5 { color: #2c3e50; }
-    h1 { font-size: 2.5em; }
-    h2 { border-bottom: 2px solid #ecf0f1; padding-bottom: 10px; }
-    h3 { border-bottom: 1px solid #ecf0f1; padding-bottom: 8px; }
-    code { background-color: #ecf0f1; padding: 2px 5px; border-radius: 4px; }
-    table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
-    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
-    th { background-color: #f2f2f2; }
-    tr:nth-child(even) { background-color: #f9f9f9; }
-    .finding-group { border: 1px solid #ccc; border-radius: 5px; padding: 15px; margin-bottom: 20px; background-color: #fafafa; }
-    .finding-card { border: 1px solid #e0e0e0; border-radius: 5px; padding: 15px; margin-bottom: 15px; background-color: #fff; }
-    img { margin-top: 10px; }
-</style>
-"""
-        return [
-            "<!DOCTYPE html>",
-            "<html lang='en'>",
-            "<head><meta charset='UTF-8'><title>Univariate Time Series Analysis Report</title>" + style + "</head>",
-            "<body>",
-            f"<h1>Scholarly Report on Univariate Time Series Analysis</h1>",
-            "<h2>Executive Summary</h2>",
-            "<p>This report presents a statistical analysis of incident frequency over time, aimed at identifying significant deviations from historical norms (anomalies) and detecting emerging patterns (trends). The dataset was disaggregated into distinct time series based on the categorical variables "
-            f"<code>{primary_col}</code> and <code>{secondary_col}</code>. Each resulting time series was modeled and evaluated independently as a separate experiment. This approach is designed to be a comprehensive screening tool, flagging every potential signal for review without applying statistical corrections for multiple comparisons. A noise reduction filter was applied to exclude statistically significant but operationally minor events, such as a single incident occurring in a category that averages less than one event per week. The findings detailed herein represent all events and trends that met the significance threshold, providing a broad set of items for further investigation.</p>",
-            "<hr>",
-            "<h2>1. Methodology</h2>",
-            "The analytical approach comprises several sequential steps: data preparation, model selection, and independent significance testing.",
-            "<h3>1.1. Data Preparation and Aggregation</h3>",
-            "<p>The raw incident data was first partitioned into subgroups based on unique combinations of the "
-            f"<code>{primary_col}</code> and <code>{secondary_col}</code> fields. For each subgroup, a time series was constructed by resampling the data into weekly incident counts. A minimum of eight weeks of data was required for a subgroup to be included in the analysis.</p>",
-            "<h3>1.2. Probabilistic Model Selection</h3>",
-            "<p>For each time series, a choice was made between the Poisson and Negative Binomial (NB) distributions to model the historical weekly counts. The model selection was based on an empirical test for overdispersion: if the variance of the historical weekly counts was greater than the mean, the more flexible Negative Binomial distribution was chosen. Otherwise, the Poisson model was used.</p>",
-            "<h3>1.3. Anomaly and Trend Detection</h3>",
-            "<p>Anomalies were identified by calculating an <strong>anomaly p-value</strong> for each of the last four weeks against the fitted historical model. To quantify the magnitude of each anomaly, a <strong>z-score</strong> was also computed. Trends were assessed by fitting a simple linear regression model to the last four weeks of data, yielding a <strong>trend p-value</strong>.</p>",
-            "<h3>1.4. Significance Testing</h3>",
-            "<p>Each time series for each subgroup is treated as an independent hypothesis test. This analysis intentionally avoids corrections for multiple comparisons (e.g., Bonferroni, FDR) to maximize sensitivity and ensure all potentially significant events are surfaced for review. The goal is to provide a comprehensive screen rather than a confirmatory analysis.</p>"
-            f"<p>A conventional significance level (alpha) of <strong>{p_thresh}</strong> is used. A finding is reported as statistically significant if its p-value is less than this threshold.</p>",
-            "<h3>1.5. Noise Reduction</h3>",
-            "<p>For time series with very low frequency (a historical average of less than one event per week), a single event can be flagged as a statistically significant anomaly. To improve the signal-to-noise ratio of the results, such findings are filtered out. An anomaly is only reported if the observed count is greater than 1, or if the historical average is 1 or greater.</p>"
-        ]
-
-    def _generate_appendix(self):
-        return [
-            "<hr><h2>Appendix: Definition of Terms</h2>",
-            "<ul>",
-            "<li><strong>Poisson Distribution</strong>: A discrete probability distribution for the counts of events that occur randomly in a given interval of time or space.</li>",
-            "<li><strong>Negative Binomial Distribution</strong>: A generalization of the Poisson distribution that allows for overdispersion, where the variance is greater than the mean.</li>",
-            "<li><strong>P-value</strong>: The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.</li>",
-            "<li><strong>Z-score</strong>: A measure of how many standard deviations an observation or data point is from the mean of a distribution. It provides a standardized measure of an anomaly's magnitude.</li>",
-            "</ul>",
-            "</body></html>"
-        ]
diff --git a/backend/reporting/stage4_reporter.py b/backend/reporting/stage4_reporter.py
deleted file mode 100644
index d499352..0000000
--- a/backend/reporting/stage4_reporter.py
+++ /dev/null
@@ -1,65 +0,0 @@
-import pandas as pd
-import os
-from typing import Optional
-
-class Stage4Reporter:
-    """
-    Generates an HTML report for Stage 4 by populating a static template
-    which uses JavaScript to load and render the results dynamically.
-    """
-
-    @property
-    def file_extension(self) -> str:
-        return "html"
-
-    def generate_report(self, data: dict, df: Optional[pd.DataFrame] = None) -> str:
-        params = data.get('parameters', {})
-        h3_resolution = params.get('h3_resolution', 'N/A')
-        secondary_col_name = params.get('secondary_group_col', 'Secondary Group')
-        p_value_threshold = 0.05
-
-        # Determine the path to the template file
-        # Assumes the template is in a 'templates' subdirectory next to this file
-        template_path = os.path.join(os.path.dirname(__file__), 'templates', 'stage4_report_template.html')
-
-        try:
-            with open(template_path, 'r') as f:
-                template_str = f.read()
-        except FileNotFoundError:
-            return "<h1>Error</h1><p>Report template not found.</p>"
-
-        # Generate methodology and appendix content
-        methodology = self._generate_methodology(secondary_col_name, h3_resolution, p_value_threshold)
-        appendix = self._generate_appendix()
-
-        # Replace placeholders in the template
-        report_content = template_str.replace('{{METHODOLOGY}}', "\n".join(methodology))
-        report_content = report_content.replace('{{APPENDIX}}', "\n".join(appendix))
-
-        return report_content
-
-    def _generate_methodology(self, secondary_col, h3_res, p_thresh):
-        return [
-            "<h2>1. Methodology</h2>",
-            "The analytical approach comprises several sequential steps: spatial aggregation, data preparation, model selection, and independent significance testing.",
-            "<h3>1.1. Spatial Aggregation and Data Preparation</h3>",
-            f"<p>Raw incident data points were assigned to a hexagonal H3 cell based on their latitude and longitude coordinates, using H3 resolution <code>{h3_res}</code>. The data was then further partitioned into subgroups based on unique combinations of the H3 cell and the "
-            f"<code>{secondary_col}</code> field. For each subgroup, a time series was constructed by resampling the data into weekly incident counts. A minimum of eight weeks of data was required for a subgroup to be included in the analysis.</p>",
-            "<h3>1.2. Probabilistic Model Selection and Anomaly/Trend Detection</h3>",
-            "<p>The methodology for model selection (Poisson vs. Negative Binomial), anomaly detection (p-value and z-score for the last four weeks), and trend detection (linear regression on the last four weeks) is identical to that used in the standard univariate analysis. Please refer to that report for detailed descriptions.</p>",
-            "<h3>1.3. Significance Testing and Noise Reduction</h3>",
-            f"<p>Each time series is treated as an independent hypothesis test. A conventional significance level (alpha) of <strong>{p_thresh}</strong> is used. A finding is reported as statistically significant if its p-value is less than this threshold. To reduce noise, anomalies are only reported for low-frequency series if the observed count is greater than 1.</p>"
-        ]
-
-    def _generate_appendix(self):
-        return [
-            "<hr><h2>Appendix: Definition of Terms</h2>",
-            "<ul>",
-            "<li><strong>H3</strong>: A geospatial indexing system that partitions the world into hexagonal cells. It allows for efficient spatial analysis at various resolutions.</li>",
-            "<li><strong>Leaflet.js</strong>: An open-source JavaScript library for mobile-friendly interactive maps.</li>",
-            "<li><strong>Poisson Distribution</strong>: A discrete probability distribution for the counts of events that occur randomly in a given interval of time or space.</li>",
-            "<li><strong>Negative Binomial Distribution</strong>: A generalization of the Poisson distribution that allows for overdispersion, where the variance is greater than the mean.</li>",
-            "<li><strong>P-value</strong>: The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.</li>",
-            "<li><strong>Z-score</strong>: A measure of how many standard deviations an observation or data point is from the mean of a distribution. It provides a standardized measure of an anomaly's magnitude.</li>",
-            "</ul>"
-        ]
diff --git a/backend/reporting/templates/stage4_report_template.html b/backend/reporting/templates/stage4_report_template.html
deleted file mode 100644
index 19b307e..0000000
--- a/backend/reporting/templates/stage4_report_template.html
+++ /dev/null
@@ -1,225 +0,0 @@
-<!DOCTYPE html>
-<html lang="en">
-<head>
-    <meta charset="UTF-8">
-    <title>H3-Based Spatial Time Series Analysis Report</title>
-    <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" />
-    <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"></script>
-    <script src="https://unpkg.com/h3-js@4.1.0/dist/h3-js.umd.js"></script>
-    <style>
-        body { font-family: sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px; }
-        h1, h2, h3, h4, h5 { color: #2c3e50; }
-        h2 { border-bottom: 2px solid #ecf0f1; padding-bottom: 10px; }
-        h3 { border-bottom: 1px solid #ecf0f1; padding-bottom: 8px; }
-        code { background-color: #ecf0f1; padding: 2px 5px; border-radius: 4px; }
-        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
-        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
-        th { background-color: #f2f2f2; }
-        tr:nth-child(even) { background-color: #f9f9f9; }
-        .finding-group { border: 1px solid #ccc; border-radius: 5px; padding: 15px; margin-bottom: 20px; background-color: #fafafa; }
-        .finding-card { border: 1px solid #e0e0e0; border-radius: 5px; padding: 15px; margin-bottom: 15px; background-color: #fff; }
-        img { margin-top: 10px; max-width: 100%; }
-        #map { height: 500px; border: 1px solid #ddd; border-radius: 5px; margin-bottom: 20px; }
-        .loader { text-align: center; padding: 40px; font-size: 1.2em; }
-    </style>
-</head>
-<body>
-    <h1>Scholarly Report on H3-Based Spatial Time Series Analysis</h1>
-    <div id="methodology-container">
-        <!-- Methodology will be injected here by the reporter -->
-        {{METHODOLOGY}}
-    </div>
-
-    <h2>2. Spatial Overview of Findings</h2>
-    <p>The following interactive map displays all H3 hexagonal cells that contained at least one significant finding (anomaly or trend). Click on a hexagon for a summary of its findings.</p>
-    <div id="map"></div>
-
-    <h2>3. Summary of All Significant Findings</h2>
-    <div id="summary-table-container">
-        <div class="loader">Loading findings...</div>
-    </div>
-
-    <h2>4. Detailed Analysis by H3 Cell</h2>
-    <div id="detailed-findings-container">
-        <div class="loader">Loading details...</div>
-    </div>
-
-    <div id="appendix-container">
-        <!-- Appendix will be injected here by the reporter -->
-        {{APPENDIX}}
-    </div>
-
-<script>
-document.addEventListener('DOMContentLoaded', function () {
-    const P_VALUE_THRESHOLD = 0.05;
-    // The JSON data will be embedded here by the reporter
-    const analysisData = '{{JSON_DATA}}';
-
-    const map = L.map('map').setView([42.3601, -71.0589], 12); // Default center
-    L.tileLayer('https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png', {
-        attribution: '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors &copy; <a href="https://carto.com/attributions">CARTO</a>'
-    }).addTo(map);
-
-    try {
-        const data = analysisData;
-        const params = data.parameters;
-        const h3Col = `h3_index_${params.h3_resolution}`;
-        const secondaryCol = params.secondary_group_col;
-
-        // --- Identify all significant findings ---
-        const allFindings = [];
-        data.results.forEach(row => {
-            const trendP = row.trend_analysis.p_value;
-            if (trendP !== null && trendP < P_VALUE_THRESHOLD) {
-                    allFindings.push({ type: 'Trend', details: row });
-                }
-                row.last_4_weeks_analysis.forEach(week => {
-                    const anomalyP = week.anomaly_p_value;
-                    if (anomalyP !== null && anomalyP < P_VALUE_THRESHOLD) {
-                        if (row.historical_weekly_avg < 1 && week.count === 1) return;
-                        allFindings.push({ type: 'Anomaly', details: row, week_details: week });
-                    }
-                });
-            });
-
-            if (allFindings.length === 0) {
-                document.getElementById('summary-table-container').innerHTML = "<p>No statistically significant trends or anomalies were detected.</p>";
-                document.getElementById('detailed-findings-container').innerHTML = "";
-                return;
-            }
-            
-            // --- Update Map ---
-            updateMap(allFindings, h3Col, secondaryCol);
-            
-            // --- Update Summary Table ---
-            populateSummaryTable(allFindings, h3Col, secondaryCol);
-
-            // --- Update Detailed Findings ---
-            populateDetailedFindings(allFindings, data.city_wide_results, h3Col, secondaryCol);
-        })
-        .catch(error => {
-            console.error('Error loading or processing analysis data:', error);
-            document.getElementById('summary-table-container').innerHTML = `<p style="color:red;">Error loading report data: ${error.message}</p>`;
-            document.getElementById('detailed-findings-container').innerHTML = "";
-        });
-
-    function updateMap(findings, h3Col, secondaryCol) {
-        const h3Summary = {};
-        findings.forEach(finding => {
-            const details = finding.details;
-            const h3Index = details[h3Col];
-            if (!h3Summary[h3Index]) {
-                h3Summary[h3Index] = { trends: [], anomalies: [], lat: details.lat, lon: details.lon };
-            }
-            if (finding.type === 'Trend') h3Summary[h3Index].trends.push(details);
-            else h3Summary[h3Index].anomalies.push(finding);
-        });
-
-        const allLats = findings.map(f => f.details.lat);
-        const allLons = findings.map(f => f.details.lon);
-        if (allLats.length > 0) {
-            const avgLat = allLats.reduce((a, b) => a + b, 0) / allLats.length;
-            const avgLon = allLons.reduce((a, b) => a + b, 0) / allLons.length;
-            map.setView([avgLat, avgLon], 12);
-        }
-
-        Object.keys(h3Summary).forEach(h3Index => {
-            const summary = h3Summary[h3Index];
-            const boundary = h3.cellToBoundary(h3Index, true); // Get [lon, lat] pairs
-            const leafletBoundary = boundary.map(p => [p[1], p[0]]); // Convert to [lat, lon] for Leaflet
-
-            const numTrends = summary.trends.length;
-            const numAnomalies = summary.anomalies.length;
-
-            let popupHtml = `<b>Hexagon:</b> ${h3Index}<br><b>Trends:</b> ${numTrends}<br><b>Anomalies:</b> ${numAnomalies}<hr>`;
-            if (numTrends > 0) {
-                popupHtml += "<b>Trends:</b><ul>";
-                summary.trends.forEach(t => popupHtml += `<li>${t[secondaryCol]}: ${t.trend_analysis.description} (p=${t.trend_analysis.p_value.toPrecision(2)})</li>`);
-                popupHtml += "</ul>";
-            }
-            if (numAnomalies > 0) {
-                popupHtml += "<b>Anomalies:</b><ul>";
-                summary.anomalies.forEach(a => popupHtml += `<li>${a.details[secondaryCol]} on ${a.week_details.week}: Count ${a.week_details.count} (p=${a.week_details.anomaly_p_value.toPrecision(2)})</li>`);
-                popupHtml += "</ul>";
-            }
-
-            const fillColor = numAnomalies > 0 ? 'red' : 'blue';
-            const fillOpacity = (numAnomalies + numTrends) > 0 ? 0.6 : 0.2;
-
-            L.polygon(leafletBoundary, { color: 'black', weight: 1, fillColor, fillOpacity })
-                .addTo(map)
-                .bindPopup(popupHtml)
-                .bindTooltip(`${h3Index}: ${numTrends} trend(s), ${numAnomalies} anomaly(s)`);
-        });
-    }
-
-    function populateSummaryTable(findings, h3Col, secondaryCol) {
-        findings.sort((a, b) => {
-            const pValA = a.type === 'Trend' ? a.details.trend_analysis.p_value : a.week_details.anomaly_p_value;
-            const pValB = b.type === 'Trend' ? b.details.trend_analysis.p_value : b.week_details.anomaly_p_value;
-            return a.details[h3Col].localeCompare(b.details[h3Col]) || pValA - pValB;
-        });
-
-        let tableHtml = `<table><thead><tr><th>H3 Cell</th><th>${secondaryCol}</th><th>Finding Type</th><th>Date/Period</th><th>Details</th><th>P-Value</th><th>Z-Score / Slope</th></tr></thead><tbody>`;
-        findings.forEach(finding => {
-            const details = finding.details;
-            tableHtml += `<tr><td><a href='#${details[h3Col]}'>${details[h3Col]}</a></td><td>${details[secondaryCol]}</td>`;
-            if (finding.type === 'Trend') {
-                const trend = details.trend_analysis;
-                tableHtml += `<td>Trend</td><td>Last 4 Weeks</td><td>${trend.description}</td><td>${trend.p_value.toPrecision(4)}</td><td>${trend.slope.toFixed(2)}</td>`;
-            } else {
-                const week = finding.week_details;
-                tableHtml += `<td>Anomaly</td><td>${week.week}</td><td>Count: ${week.count} (vs avg ${details.historical_weekly_avg.toFixed(2)})</td><td>${week.anomaly_p_value.toPrecision(4)}</td><td>${week.z_score.toFixed(2)}</td>`;
-            }
-            tableHtml += "</tr>";
-        });
-        tableHtml += '</tbody></table>';
-        document.getElementById('summary-table-container').innerHTML = tableHtml;
-    }
-
-    function populateDetailedFindings(findings, cityWideResults, h3Col, secondaryCol) {
-        const findingsByH3 = {};
-        findings.forEach(f => {
-            const h3Index = f.details[h3Col];
-            if (!findingsByH3[h3Index]) findingsByH3[h3Index] = [];
-            findingsByH3[h3Index].push(f);
-        });
-
-        const cityWideMap = {};
-        cityWideResults.forEach(item => cityWideMap[item[secondaryCol]] = item);
-
-        let detailedHtml = '';
-        Object.keys(findingsByH3).sort().forEach(h3Index => {
-            detailedHtml += `<div class="finding-group" id="${h3Index}"><h4>Hexagon: ${h3Index}</h4>`;
-            findingsByH3[h3Index].forEach(finding => {
-                const details = finding.details;
-                const secGroup = details[secondaryCol];
-                
-                detailedHtml += `<div class="finding-card">`;
-                if (finding.type === 'Trend') {
-                    const trend = details.trend_analysis;
-                    detailedHtml += `<h5>Finding: Trend in '${secGroup}'</h5><ul>
-                        <li><strong>Description</strong>: ${trend.description}</li>
-                        <li><strong>Weekly Change (Slope)</strong>: ${trend.slope.toFixed(2)}</li>
-                        <li><strong>Significance (p-value)</strong>: ${trend.p_value.toPrecision(4)}</li></ul>`;
-                } else {
-                    const week = finding.week_details;
-                    detailedHtml += `<h5>Finding: Anomaly in '${secGroup}'</h5><ul>
-                        <li><strong>Date</strong>: ${week.week}</li>
-                        <li><strong>Observed Count</strong>: ${week.count} (Historical Avg: ${details.historical_weekly_avg.toFixed(2)})</li>
-                        <li><strong>Magnitude (Z-Score)</strong>: ${week.z_score.toFixed(2)}</li>
-                        <li><strong>Significance (p-value)</strong>: ${week.anomaly_p_value.toPrecision(4)}</li></ul>`;
-                }
-                // Note: Plot generation is not included in this client-side version for simplicity.
-                // To include plots, they would need to be generated and saved by the Python process,
-                // and this script would reference them by a predictable filename.
-                detailedHtml += '</div>';
-            });
-            detailedHtml += '</div>';
-        });
-        document.getElementById('detailed-findings-container').innerHTML = detailedHtml;
-    }
-});
-</script>
-</body>
-</html>
diff --git a/backend/reporting/visualizations/__init__.py b/backend/reporting/visualizations/__init__.py
deleted file mode 100644
index e1a4f14..0000000
--- a/backend/reporting/visualizations/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-# This file makes the visualizations directory a Python package.
diff --git a/backend/reporting/visualizations/common.py b/backend/reporting/visualizations/common.py
deleted file mode 100644
index ec43faa..0000000
--- a/backend/reporting/visualizations/common.py
+++ /dev/null
@@ -1,105 +0,0 @@
-import matplotlib.pyplot as plt
-import pandas as pd
-import os
-from typing import Optional
-
-def plot_raw_and_aggregated_data(df: pd.DataFrame, timestamp_col: str, output_dir: str) -> str:
-    """
-    Generates overview plots: weekly counts, and distributions by day of week and hour of day.
-    """
-    plt.style.use('seaborn-v0_8-whitegrid')
-    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 18))
-
-    # 1. Plot weekly aggregated data
-    weekly_counts = df.resample('W', on=timestamp_col).size()
-    ax1.plot(weekly_counts.index, weekly_counts.values, 'o-', label='Weekly Total Incidents', color='teal')
-    ax1.set_title("Weekly Aggregated Incident Counts (All Data)", fontsize=14)
-    ax1.set_ylabel("Incident Count")
-    ax1.legend()
-    ax1.tick_params(axis='x', rotation=45)
-
-    # 2. Plot distribution by day of the week
-    day_of_week_counts = df[timestamp_col].dt.day_name().value_counts()
-    days = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
-    day_of_week_counts = day_of_week_counts.reindex(days)
-    ax2.bar(day_of_week_counts.index, day_of_week_counts.values, color='skyblue')
-    ax2.set_title("Total Incidents by Day of Week", fontsize=14)
-    ax2.set_ylabel("Total Incident Count")
-    ax2.tick_params(axis='x', rotation=45)
-
-    # 3. Plot distribution by hour of the day
-    hour_of_day_counts = df[timestamp_col].dt.hour.value_counts().sort_index()
-    ax3.bar(hour_of_day_counts.index, hour_of_day_counts.values, color='salmon')
-    ax3.set_title("Total Incidents by Hour of Day", fontsize=14)
-    ax3.set_xlabel("Hour of Day (0-23)")
-    ax3.set_ylabel("Total Incident Count")
-    ax3.set_xticks(range(24))
-
-    plt.tight_layout(pad=3.0)
-    
-    filename = "plot_initial_aggregation.png"
-    filepath = os.path.join(output_dir, filename)
-    plt.savefig(filepath)
-    plt.close(fig)
-    return filename
-
-def plot_comparative_time_series(
-    group_series: pd.Series,
-    group_name: str,
-    city_wide_series: Optional[pd.Series],
-    primary_col: str,
-    secondary_col: str,
-    output_dir: str,
-    anomaly_points: Optional[list] = None,
-    filename_override: Optional[str] = None
-) -> str:
-    """
-    Generates a plot comparing a specific group's time series to the city-wide equivalent.
-    """
-    plt.style.use('seaborn-v0_8-whitegrid')
-    fig, ax = plt.subplots(figsize=(15, 7))
-
-    # Plot the specific group's data
-    ax.plot(group_series.index, group_series.values, 'o-', label=f'{primary_col}: {group_name}', color='blue', linewidth=2)
-
-    # If city-wide data is available, plot it on a secondary y-axis
-    if city_wide_series is not None:
-        ax2 = ax.twinx()
-        ax2.plot(city_wide_series.index, city_wide_series.values, 's--', label='City-Wide', color='gray', alpha=0.7)
-        ax.set_ylabel(f'Incident Count ({group_name})', color='blue')
-        ax2.set_ylabel('Incident Count (City-Wide)', color='gray')
-        ax.tick_params(axis='y', labelcolor='blue')
-        ax2.tick_params(axis='y', labelcolor='gray')
-        lines, labels = ax.get_legend_handles_labels()
-        lines2, labels2 = ax2.get_legend_handles_labels()
-        ax2.legend(lines + lines2, labels + labels2, loc=0)
-    else:
-        ax.set_ylabel('Incident Count')
-        ax.legend(loc=0)
-
-    # Highlight anomalous points on the primary series
-    if anomaly_points:
-        for point in anomaly_points:
-            ts = pd.to_datetime(point['week'])
-            y_value = point.get('deseasonalized_count', point.get('count'))
-            if y_value is not None:
-                ax.plot(ts, y_value, 'ro', markersize=12, alpha=0.8, label=f"Anomaly on {point['week']}")
-
-    ax.set_title(f"Comparison for '{secondary_col}': {group_name} vs. City-Wide", fontsize=16)
-    ax.set_xlabel("Date")
-    
-    plt.xticks(rotation=45)
-    plt.tight_layout()
-
-    if filename_override:
-        filename = filename_override
-    else:
-        safe_primary = "".join(c for c in group_name if c.isalnum())
-        safe_secondary = "".join(c for c in secondary_col if c.isalnum())[:50]
-        filename = f"plot_compare_{safe_primary}_{safe_secondary}.png"
-    
-    filepath = os.path.join(output_dir, filename)
-    
-    plt.savefig(filepath)
-    plt.close(fig)
-    return filename
diff --git a/backend/reporting/visualizations/stage3.py b/backend/reporting/visualizations/stage3.py
deleted file mode 100644
index e611c13..0000000
--- a/backend/reporting/visualizations/stage3.py
+++ /dev/null
@@ -1,92 +0,0 @@
-import matplotlib.pyplot as plt
-import pandas as pd
-import os
-import numpy as np
-
-def plot_trend_time_series(
-    weekly_counts: pd.Series,
-    trend_analysis: dict,
-    primary_col: str,
-    secondary_col: str,
-    output_dir: str
-) -> str:
-    """
-    Generates and saves a plot of the time series with a trend line for the last 4 weeks.
-    """
-    plt.style.use('seaborn-v0_8-whitegrid')
-    fig, ax = plt.subplots(figsize=(12, 6))
-
-    ax.plot(weekly_counts.index, weekly_counts.values, 'o-', label='Weekly Counts', color='gray', alpha=0.7)
-
-    if len(weekly_counts.index) >= 4:
-        last_4_weeks_start = weekly_counts.index[-4]
-        last_4_weeks_end = weekly_counts.index[-1]
-        ax.axvspan(last_4_weeks_start, last_4_weeks_end, color='yellow', alpha=0.2, label='Trend Analysis Window')
-
-        recent_counts = weekly_counts.loc[last_4_weeks_start:]
-        time_steps = np.arange(len(recent_counts))
-        slope = trend_analysis['slope']
-        intercept = recent_counts.mean() - slope * np.mean(time_steps)
-        trend_line = slope * time_steps + intercept
-        ax.plot(recent_counts.index, trend_line, 'r--', label=f"Trend (Slope: {slope:.2f})")
-
-    ax.set_title(f"Weekly Incidents & Trend: {secondary_col}\nin {primary_col}", fontsize=16)
-    ax.set_xlabel("Date")
-    ax.set_ylabel("Incident Count")
-    ax.legend()
-    plt.xticks(rotation=45)
-    plt.tight_layout()
-
-    safe_primary = "".join(c for c in primary_col if c.isalnum())
-    safe_secondary = "".join(c for c in secondary_col if c.isalnum())[:50]
-    filename = f"plot_trend_{safe_primary}_{safe_secondary}.png"
-    filepath = os.path.join(output_dir, filename)
-    
-    plt.savefig(filepath)
-    plt.close(fig)
-    return filename
-
-def plot_anomaly_time_series(
-    weekly_counts: pd.Series,
-    historical_dist,
-    anomaly_points: list,
-    primary_col: str,
-    secondary_col: str,
-    output_dir: str
-) -> str:
-    """
-    Generates and saves a plot of the time series with historical fit and anomalies.
-    """
-    plt.style.use('seaborn-v0_8-whitegrid')
-    fig, ax = plt.subplots(figsize=(12, 6))
-
-    ax.plot(weekly_counts.index, weekly_counts.values, 'o-', label='Weekly Counts', color='gray', alpha=0.7)
-
-    expected_mean = historical_dist.mean()
-    ci_upper_bound = historical_dist.ppf(0.95)
-    ax.axhline(expected_mean, color='blue', linestyle='--', label=f'Historical Mean ({expected_mean:.2f})')
-    ax.axhline(ci_upper_bound, color='red', linestyle=':', label=f'95% Confidence Bound ({ci_upper_bound:.2f})')
-
-    for point in anomaly_points:
-        ts = pd.to_datetime(point['week'])
-        ax.plot(ts, point['count'], 'ro', markersize=10, label=f"Anomaly on {point['week']}")
-
-    if not weekly_counts.empty:
-        last_4_weeks_start = weekly_counts.index[-4] if len(weekly_counts.index) >= 4 else weekly_counts.index[0]
-        ax.axvspan(last_4_weeks_start, weekly_counts.index[-1], color='yellow', alpha=0.2, label='Analysis Window')
-
-    ax.set_title(f"Weekly Incidents: {secondary_col}\nin {primary_col}", fontsize=16)
-    ax.set_xlabel("Date")
-    ax.set_ylabel("Incident Count")
-    ax.legend()
-    plt.xticks(rotation=45)
-    plt.tight_layout()
-
-    safe_primary = "".join(c for c in primary_col if c.isalnum())
-    safe_secondary = "".join(c for c in secondary_col if c.isalnum())[:50]
-    filename = f"plot_{safe_primary}_{safe_secondary}.png"
-    filepath = os.path.join(output_dir, filename)
-    
-    plt.savefig(filepath)
-    plt.close(fig)
-    return filename
diff --git a/backend/requirements.txt b/backend/requirements.txt
index 10fe463..b766f91 100644
--- a/backend/requirements.txt
+++ b/backend/requirements.txt
@@ -20,3 +20,5 @@ h3
 # Task Queue
 celery
 redis
+
+boto3
diff --git a/backend/stages/stage2_yearly_count_comparison.py b/backend/stages/stage2_yearly_count_comparison.py
index c24e1b0..b59aeee 100644
--- a/backend/stages/stage2_yearly_count_comparison.py
+++ b/backend/stages/stage2_yearly_count_comparison.py
@@ -4,6 +4,7 @@ import os
 import json
 import logging
 from typing import Optional
+from core.storage import JsonStorageModel
 
 logger = logging.getLogger(__name__)
 
@@ -21,7 +22,9 @@ class Stage2YearlyCountComparison:
         self.redis_client = redis_client
         self.data_sources = data_sources
         self.job_dir = os.path.join(results_dir, self.job_id)
+        # We still create local dir for temp processing if needed, but storage is abstracted
         os.makedirs(self.job_dir, exist_ok=True)
+        self.json_storage = JsonStorageModel()
 
     @property
     def name(self) -> str:
@@ -36,16 +39,10 @@ class Stage2YearlyCountComparison:
 
     def _save_results(self, results: dict, filename: str) -> str:
         """
-        Helper method to save a dictionary as JSON to the local results directory.
-        Returns the path to the saved file.
+        Helper method to save results using the JsonStorageModel.
         """
-        output_path = os.path.join(self.job_dir, filename)
-        
-        print(f"Saving results for job {self.job_id} to {output_path}")
-        with open(output_path, 'w') as f:
-            json.dump(results, f, indent=4)
-            
-        return output_path
+        print(f"Saving results for job {self.job_id} to storage: {filename}")
+        return self.json_storage.save(self.job_id, filename, results)
 
     def run(self, df: Optional[pd.DataFrame] = None) -> dict:
         """
@@ -53,11 +50,11 @@ class Stage2YearlyCountComparison:
         """
         # Check if stage should be skipped
         skip_existing = self.config.get('skip_existing', False)
-        output_path = os.path.join(self.job_dir, f"{self.name}.json")
-        if skip_existing and os.path.exists(output_path):
+        filename = f"{self.name}.json"
+        
+        if skip_existing and self.json_storage.exists(self.job_id, filename):
             print(f"Skipping stage {self.name} as output already exists.")
-            with open(output_path, 'r') as f:
-                return json.load(f)
+            return self.json_storage.load(self.job_id, filename)
 
         # --- This stage loads its own data ---
         if not self.data_sources:
@@ -142,5 +139,5 @@ class Stage2YearlyCountComparison:
             "all_years": all_years
         }
 
-        self._save_results(output, f"{self.name}.json")
+        self._save_results(output, filename)
         return output
diff --git a/backend/stages/stage3_univariate_anomaly.py b/backend/stages/stage3_univariate_anomaly.py
index 3a9ab9b..dbf2402 100644
--- a/backend/stages/stage3_univariate_anomaly.py
+++ b/backend/stages/stage3_univariate_anomaly.py
@@ -4,11 +4,431 @@ from scipy import stats
 import os
 import json
 import logging
-from reporting.stage3_reporter import Stage3Reporter
+import matplotlib.pyplot as plt
 from typing import Optional, Tuple
+from core.storage import JsonStorageModel, ImageStorageModel
 
 logger = logging.getLogger(__name__)
 
+# --- Visualization Functions (Updated to return Figure objects instead of saving) ---
+
+def plot_raw_and_aggregated_data(df: pd.DataFrame, timestamp_col: str) -> plt.Figure:
+    """Generates overview plots."""
+    plt.style.use('seaborn-v0_8-whitegrid')
+    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 18))
+
+    # 1. Plot weekly aggregated data
+    weekly_counts = df.resample('W', on=timestamp_col).size()
+    ax1.plot(weekly_counts.index, weekly_counts.values, 'o-', label='Weekly Total Incidents', color='teal')
+    ax1.set_title("Weekly Aggregated Incident Counts (All Data)", fontsize=14)
+    ax1.set_ylabel("Incident Count")
+    ax1.legend()
+    ax1.tick_params(axis='x', rotation=45)
+
+    # 2. Plot distribution by day of the week
+    day_of_week_counts = df[timestamp_col].dt.day_name().value_counts()
+    days = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
+    day_of_week_counts = day_of_week_counts.reindex(days)
+    ax2.bar(day_of_week_counts.index, day_of_week_counts.values, color='skyblue')
+    ax2.set_title("Total Incidents by Day of Week", fontsize=14)
+    ax2.set_ylabel("Total Incident Count")
+    ax2.tick_params(axis='x', rotation=45)
+
+    # 3. Plot distribution by hour of the day
+    hour_of_day_counts = df[timestamp_col].dt.hour.value_counts().sort_index()
+    ax3.bar(hour_of_day_counts.index, hour_of_day_counts.values, color='salmon')
+    ax3.set_title("Total Incidents by Hour of Day", fontsize=14)
+    ax3.set_xlabel("Hour of Day (0-23)")
+    ax3.set_ylabel("Total Incident Count")
+    ax3.set_xticks(range(24))
+
+    plt.tight_layout(pad=3.0)
+    return fig
+
+def plot_comparative_time_series(
+    group_series: pd.Series,
+    group_name: str,
+    city_wide_series: Optional[pd.Series],
+    primary_col: str,
+    secondary_col: str,
+    anomaly_points: Optional[list] = None
+) -> plt.Figure:
+    """Generates a plot comparing a specific group's time series to the city-wide equivalent."""
+    plt.style.use('seaborn-v0_8-whitegrid')
+    fig, ax = plt.subplots(figsize=(15, 7))
+
+    # Plot the specific group's data
+    ax.plot(group_series.index, group_series.values, 'o-', label=f'{primary_col}: {group_name}', color='blue', linewidth=2)
+
+    # If city-wide data is available, plot it on a secondary y-axis
+    if city_wide_series is not None:
+        ax2 = ax.twinx()
+        ax2.plot(city_wide_series.index, city_wide_series.values, 's--', label='City-Wide', color='gray', alpha=0.7)
+        ax.set_ylabel(f'Incident Count ({group_name})', color='blue')
+        ax2.set_ylabel('Incident Count (City-Wide)', color='gray')
+        ax.tick_params(axis='y', labelcolor='blue')
+        ax2.tick_params(axis='y', labelcolor='gray')
+        lines, labels = ax.get_legend_handles_labels()
+        lines2, labels2 = ax2.get_legend_handles_labels()
+        ax2.legend(lines + lines2, labels + labels2, loc=0)
+    else:
+        ax.set_ylabel('Incident Count')
+        ax.legend(loc=0)
+
+    # Highlight anomalous points on the primary series
+    if anomaly_points:
+        for point in anomaly_points:
+            ts = pd.to_datetime(point['week'])
+            y_value = point.get('deseasonalized_count', point.get('count'))
+            if y_value is not None:
+                ax.plot(ts, y_value, 'ro', markersize=12, alpha=0.8, label=f"Anomaly on {point['week']}")
+
+    ax.set_title(f"Comparison for '{secondary_col}': {group_name} vs. City-Wide", fontsize=16)
+    ax.set_xlabel("Date")
+    
+    plt.xticks(rotation=45)
+    plt.tight_layout()
+    return fig
+
+def plot_trend_time_series(
+    weekly_counts: pd.Series,
+    trend_analysis: dict,
+    primary_col: str,
+    secondary_col: str,
+    output_dir: str
+) -> str:
+    """Generates and saves a plot of the time series with a trend line for the last 4 weeks."""
+    plt.style.use('seaborn-v0_8-whitegrid')
+    fig, ax = plt.subplots(figsize=(12, 6))
+
+    ax.plot(weekly_counts.index, weekly_counts.values, 'o-', label='Weekly Counts', color='gray', alpha=0.7)
+
+    if len(weekly_counts.index) >= 4:
+        last_4_weeks_start = weekly_counts.index[-4]
+        last_4_weeks_end = weekly_counts.index[-1]
+        ax.axvspan(last_4_weeks_start, last_4_weeks_end, color='yellow', alpha=0.2, label='Trend Analysis Window')
+
+        recent_counts = weekly_counts.loc[last_4_weeks_start:]
+        time_steps = np.arange(len(recent_counts))
+        slope = trend_analysis['slope']
+        intercept = recent_counts.mean() - slope * np.mean(time_steps)
+        trend_line = slope * time_steps + intercept
+        ax.plot(recent_counts.index, trend_line, 'r--', label=f"Trend (Slope: {slope:.2f})")
+
+    ax.set_title(f"Weekly Incidents & Trend: {secondary_col}\nin {primary_col}", fontsize=16)
+    ax.set_xlabel("Date")
+    ax.set_ylabel("Incident Count")
+    ax.legend()
+    plt.xticks(rotation=45)
+    plt.tight_layout()
+
+    safe_primary = "".join(c for c in primary_col if c.isalnum())
+    safe_secondary = "".join(c for c in secondary_col if c.isalnum())[:50]
+    filename = f"plot_trend_{safe_primary}_{safe_secondary}.png"
+    filepath = os.path.join(output_dir, filename)
+    
+    plt.savefig(filepath)
+    plt.close(fig)
+    return filename
+
+def plot_anomaly_time_series(
+    weekly_counts: pd.Series,
+    historical_dist,
+    anomaly_points: list,
+    primary_col: str,
+    secondary_col: str,
+    output_dir: str
+) -> str:
+    """Generates and saves a plot of the time series with historical fit and anomalies."""
+    plt.style.use('seaborn-v0_8-whitegrid')
+    fig, ax = plt.subplots(figsize=(12, 6))
+
+    ax.plot(weekly_counts.index, weekly_counts.values, 'o-', label='Weekly Counts', color='gray', alpha=0.7)
+
+    expected_mean = historical_dist.mean()
+    ci_upper_bound = historical_dist.ppf(0.95)
+    ax.axhline(expected_mean, color='blue', linestyle='--', label=f'Historical Mean ({expected_mean:.2f})')
+    ax.axhline(ci_upper_bound, color='red', linestyle=':', label=f'95% Confidence Bound ({ci_upper_bound:.2f})')
+
+    for point in anomaly_points:
+        ts = pd.to_datetime(point['week'])
+        ax.plot(ts, point['count'], 'ro', markersize=10, label=f"Anomaly on {point['week']}")
+
+    if not weekly_counts.empty:
+        last_4_weeks_start = weekly_counts.index[-4] if len(weekly_counts.index) >= 4 else weekly_counts.index[0]
+        ax.axvspan(last_4_weeks_start, weekly_counts.index[-1], color='yellow', alpha=0.2, label='Analysis Window')
+
+    ax.set_title(f"Weekly Incidents: {secondary_col}\nin {primary_col}", fontsize=16)
+    ax.set_xlabel("Date")
+    ax.set_ylabel("Incident Count")
+    ax.legend()
+    plt.xticks(rotation=45)
+    plt.tight_layout()
+
+    safe_primary = "".join(c for c in primary_col if c.isalnum())
+    safe_secondary = "".join(c for c in secondary_col if c.isalnum())[:50]
+    filename = f"plot_{safe_primary}_{safe_secondary}.png"
+    filepath = os.path.join(output_dir, filename)
+    
+    plt.savefig(filepath)
+    plt.close(fig)
+    return filename
+
+# --- Reporter Class (Consolidated) ---
+
+class Stage3Reporter:
+    """
+    Generates a scholarly HTML report from the Stage 3 Univariate Anomaly results,
+    including data visualizations and robust statistical explanations.
+    """
+    def __init__(self, job_id, image_storage):
+        self.job_id = job_id
+        self.image_storage = image_storage
+
+    @property
+    def file_extension(self) -> str:
+        return "html"
+
+    def generate_report(self, data: dict, df: Optional[pd.DataFrame] = None) -> str:
+        params = data.get('parameters', {})
+        primary_col_name = params.get('primary_group_col', 'Primary Group')
+        secondary_col_name = params.get('secondary_group_col', 'Secondary Group')
+        timestamp_col_name = data.get('parameters', {}).get('timestamp_col', self._find_timestamp_col(df))
+        results = data.get('results', [])
+        city_wide_results = data.get('city_wide_results', [])
+        job_dir = os.path.dirname(data.get('__filepath__', '.'))
+
+        if not results and not city_wide_results:
+            return "<h1>Scholarly Report on Univariate Time Series Analysis</h1><p>No results were generated.</p>"
+
+        # Combine localized and city-wide results for unified processing
+        for r in results:
+            r['primary_group_name'] = r[primary_col_name]
+        
+        report_df = pd.DataFrame(results + city_wide_results)
+        
+        p_value_threshold = 0.05
+        
+        report_lines = self._generate_header_and_methodology(primary_col_name, secondary_col_name, p_value_threshold)
+
+        report_lines.append("<h2>2. Analysis by Group</h2>")
+
+        if df is not None and timestamp_col_name:
+            report_lines.append("<h3>2.1 Overall Data Overview</h3>")
+            report_lines.append("<p>The following plot shows the weekly aggregated counts and distributions by day and hour for the entire dataset. This provides a high-level context for the detailed findings below.</p>")
+            
+            fig = plot_raw_and_aggregated_data(df, timestamp_col_name)
+            filename = "plot_initial_aggregation.png"
+            self.image_storage.save_plot(self.job_id, filename, fig)
+            
+            # In S3 mode, we might need a signed URL, but for now assume relative path works with API proxy
+            report_lines.append(f'<img src="results/{filename}" alt="Initial Data Aggregation" style="width:100%; max-width:800px;">')
+            report_lines.append("<h3>2.2 Summary of Significant Findings</h3>")
+        else:
+            report_lines.append("<h3>2.1 Summary of Significant Findings</h3>")
+
+        all_findings = []
+        for _, row in report_df.iterrows():
+            trend_p = row['trend_analysis']['p_value']
+            if trend_p is not None and trend_p < p_value_threshold:
+                all_findings.append({'type': 'Trend', 'group': row['primary_group_name'], 'details': row})
+            
+            for week in row['last_4_weeks_analysis']:
+                anomaly_p = week['anomaly_p_value']
+                if anomaly_p is not None and anomaly_p < p_value_threshold:
+                    if row['historical_weekly_avg'] < 1 and week['count'] == 1:
+                        continue
+                    finding = {'type': 'Anomaly', 'group': row['primary_group_name'], 'details': row.to_dict(), 'week_details': week}
+                    all_findings.append(finding)
+
+        if not all_findings:
+            report_lines.append("<p>No statistically significant trends or anomalies were detected.</p>")
+        else:
+            city_wide_map = {item[secondary_col_name]: item for item in city_wide_results}
+            
+            # Sort all findings by primary group, then by p-value
+            sorted_findings = sorted(all_findings, key=lambda x: (
+                x['group'], 
+                x['details']['trend_analysis']['p_value'] if x['type'] == 'Trend' else x['week_details']['anomaly_p_value']
+            ))
+
+            # --- Generate Summary Table ---
+            report_lines.append('<table><thead><tr>'
+                                f'<th>{primary_col_name}</th><th>{secondary_col_name}</th><th>Finding Type</th><th>Date/Period</th>'
+                                '<th>Details</th><th>P-Value</th><th>Z-Score / Slope</th>'
+                                '</tr></thead><tbody>')
+            for finding in sorted_findings:
+                details = finding['details']
+                row_html = f"<tr><td>{details['primary_group_name']}</td><td>{details[secondary_col_name]}</td>"
+                if finding['type'] == 'Trend':
+                    trend = details['trend_analysis']
+                    p_val = trend['p_value']
+                    row_html += f"<td>Trend</td><td>Last 4 Weeks</td><td>{trend['description']}</td><td>{p_val:.4g}</td><td>{trend['slope']:.2f}</td>"
+                else: # Anomaly
+                    week = finding['week_details']
+                    p_val = week['anomaly_p_value']
+                    row_html += f"<td>Anomaly</td><td>{week['week']}</td><td>Count: {week['count']} (vs avg {details['historical_weekly_avg']:.2f})</td><td>{p_val:.4g}</td><td>{week['z_score']:.2f}</td>"
+                row_html += "</tr>"
+                report_lines.append(row_html)
+            report_lines.append('</tbody></table>')
+
+            # --- Detailed Breakdown, Grouped by Primary Column ---
+            if df is not None and timestamp_col_name:
+                report_lines.append("<h3>2.3 Detailed Analysis of Findings</h3>")
+            else:
+                report_lines.append("<h3>2.2 Detailed Analysis of Findings</h3>")
+
+            current_group = None
+            for i, finding in enumerate(sorted_findings):
+                details = finding['details']
+                group_name = details['primary_group_name']
+                
+                if group_name != current_group:
+                    if current_group is not None: report_lines.append("</div>") # Close previous group div
+                    report_lines.append(f'<div class="finding-group"><h4>Group: {group_name}</h4>')
+                    current_group = group_name
+
+                sec_group = details[secondary_col_name]
+                city_wide_data = city_wide_map.get(sec_group)
+                
+                finding_id = f"finding-{i+1}"
+                report_lines.append(f'<div class="finding-card" id="{finding_id}">')
+
+                if finding['type'] == 'Trend':
+                    trend_details = details['trend_analysis']
+                    p_val = trend_details['p_value']
+                    report_lines.append(f"<h5>Finding {i+1}: Trend in '{sec_group}' for {details['primary_group_name']}</h5><ul>")
+                    report_lines.append(f"<li><strong>Description</strong>: {trend_details['description']}</li>")
+                    report_lines.append(f"<li><strong>Weekly Change (Slope)</strong>: {trend_details['slope']:.2f}</li>")
+                    report_lines.append(f"<li><strong>Significance (p-value)</strong>: {p_val:.4g}</li>")
+                    
+                    if city_wide_data and details['primary_group_name'] != 'City-Wide':
+                        cw_trend = city_wide_data['trend_analysis']
+                        report_lines.append(f"<li><strong>City-Wide Context</strong>: The trend for '{sec_group}' across all groups is: <strong>{cw_trend['description']}</strong> (p-value: {cw_trend['p_value']:.4g}, slope: {cw_trend['slope']:.2f}).</li>")
+                    report_lines.append("</ul>")
+                    plot_filename = self._generate_comparative_plot(details, city_wide_data, primary_col_name, secondary_col_name)
+                    report_lines.append(f'<img src="results/{plot_filename}" alt="Time series for {sec_group}" style="width:100%; max-width:600px;">')
+
+                else: # Anomaly
+                    week_details = finding['week_details']
+                    p_val = week_details['anomaly_p_value']
+                    report_lines.append(f"<h5>Finding {i+1}: Anomaly in '{sec_group}' for {details['primary_group_name']}</h5><ul>")
+                    report_lines.append(f"<li><strong>Date</strong>: {week_details['week']}</li>")
+                    report_lines.append(f"<li><strong>Observed Count</strong>: {week_details['count']} (Historical Avg: {details['historical_weekly_avg']:.2f})</li>")
+                    report_lines.append(f"<li><strong>Magnitude (Z-Score)</strong>: {week_details['z_score']:.2f}</li>")
+                    report_lines.append(f"<li><strong>Significance (p-value)</strong>: {p_val:.4g}</li>")
+                    
+                    if city_wide_data and details['primary_group_name'] != 'City-Wide':
+                        cw_week_data = next((w for w in city_wide_data['last_4_weeks_analysis'] if w['week'] == week_details['week']), None)
+                        if cw_week_data:
+                            cw_p_val = cw_week_data['anomaly_p_value']
+                            status = "significant" if cw_p_val < p_value_threshold else "not significant"
+                            report_lines.append(f"<li><strong>City-Wide Context</strong>: The same week was <strong>{status}</strong> for '{sec_group}' across all groups (p-value: {cw_p_val:.4g}).</li>")
+                    report_lines.append("</ul>")
+                    plot_filename = self._generate_comparative_plot(details, city_wide_data, primary_col_name, secondary_col_name, anomaly_points=[week_details])
+                    report_lines.append(f'<img src="results/{plot_filename}" alt="Time series for {sec_group}" style="width:100%; max-width:600px;">')
+                
+                report_lines.append('</div>') # Close finding-card
+            
+            if current_group is not None: report_lines.append("</div>") # Close final group div
+
+        report_lines.extend(self._generate_appendix())
+        return "\n".join(report_lines)
+
+    def _find_timestamp_col(self, df: Optional[pd.DataFrame]) -> Optional[str]:
+        """Helper to find a timestamp column if not explicitly provided."""
+        if df is None:
+            return None
+        for col in df.columns:
+            if pd.api.types.is_datetime64_any_dtype(df[col]):
+                return col
+        return None
+
+    def _generate_comparative_plot(self, group_data: dict, city_wide_data: Optional[dict], primary_col: str, secondary_col: str, anomaly_points: Optional[list] = None) -> str:
+        """Helper to generate and save a comparative plot."""
+        group_series = pd.Series(group_data['full_weekly_series'])
+        group_series.index = pd.to_datetime(group_series.index)
+
+        city_wide_series = None
+        if city_wide_data and group_data['primary_group_name'] != 'City-Wide':
+            city_wide_series = pd.Series(city_wide_data['full_weekly_series'])
+            city_wide_series.index = pd.to_datetime(city_wide_series.index)
+
+        fig = plot_comparative_time_series(
+            group_series=group_series,
+            group_name=group_data['primary_group_name'],
+            city_wide_series=city_wide_series,
+            primary_col=primary_col,
+            secondary_col=group_data[secondary_col],
+            anomaly_points=anomaly_points
+        )
+        
+        safe_primary = "".join(c for c in group_data['primary_group_name'] if c.isalnum())
+        safe_secondary = "".join(c for c in group_data[secondary_col] if c.isalnum())[:50]
+        filename = f"plot_compare_{safe_primary}_{safe_secondary}.png"
+        
+        self.image_storage.save_plot(self.job_id, filename, fig)
+        return filename
+
+    def _generate_header_and_methodology(self, primary_col, secondary_col, p_thresh):
+        # Basic CSS for better readability
+        style = """
+<style>
+    body { font-family: sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px; }
+    h1, h2, h3, h4, h5 { color: #2c3e50; }
+    h1 { font-size: 2.5em; }
+    h2 { border-bottom: 2px solid #ecf0f1; padding-bottom: 10px; }
+    h3 { border-bottom: 1px solid #ecf0f1; padding-bottom: 8px; }
+    code { background-color: #ecf0f1; padding: 2px 5px; border-radius: 4px; }
+    table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
+    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
+    th { background-color: #f2f2f2; }
+    tr:nth-child(even) { background-color: #f9f9f9; }
+    .finding-group { border: 1px solid #ccc; border-radius: 5px; padding: 15px; margin-bottom: 20px; background-color: #fafafa; }
+    .finding-card { border: 1px solid #e0e0e0; border-radius: 5px; padding: 15px; margin-bottom: 15px; background-color: #fff; }
+    img { margin-top: 10px; }
+</style>
+"""
+        return [
+            "<!DOCTYPE html>",
+            "<html lang='en'>",
+            "<head><meta charset='UTF-8'><title>Univariate Time Series Analysis Report</title>" + style + "</head>",
+            "<body>",
+            f"<h1>Scholarly Report on Univariate Time Series Analysis</h1>",
+            "<h2>Executive Summary</h2>",
+            "<p>This report presents a statistical analysis of incident frequency over time, aimed at identifying significant deviations from historical norms (anomalies) and detecting emerging patterns (trends). The dataset was disaggregated into distinct time series based on the categorical variables "
+            f"<code>{primary_col}</code> and <code>{secondary_col}</code>. Each resulting time series was modeled and evaluated independently as a separate experiment. This approach is designed to be a comprehensive screening tool, flagging every potential signal for review without applying statistical corrections for multiple comparisons. A noise reduction filter was applied to exclude statistically significant but operationally minor events, such as a single incident occurring in a category that averages less than one event per week. The findings detailed herein represent all events and trends that met the significance threshold, providing a broad set of items for further investigation.</p>",
+            "<hr>",
+            "<h2>1. Methodology</h2>",
+            "The analytical approach comprises several sequential steps: data preparation, model selection, and independent significance testing.",
+            "<h3>1.1. Data Preparation and Aggregation</h3>",
+            "<p>The raw incident data was first partitioned into subgroups based on unique combinations of the "
+            f"<code>{primary_col}</code> and <code>{secondary_col}</code> fields. For each subgroup, a time series was constructed by resampling the data into weekly incident counts. A minimum of eight weeks of data was required for a subgroup to be included in the analysis.</p>",
+            "<h3>1.2. Probabilistic Model Selection</h3>",
+            "<p>For each time series, a choice was made between the Poisson and Negative Binomial (NB) distributions to model the historical weekly counts. The model selection was based on an empirical test for overdispersion: if the variance of the historical weekly counts was greater than the mean, the more flexible Negative Binomial distribution was chosen. Otherwise, the Poisson model was used.</p>",
+            "<h3>1.3. Anomaly and Trend Detection</h3>",
+            "<p>Anomalies were identified by calculating an <strong>anomaly p-value</strong> for each of the last four weeks against the fitted historical model. To quantify the magnitude of each anomaly, a <strong>z-score</strong> was also computed. Trends were assessed by fitting a simple linear regression model to the last four weeks of data, yielding a <strong>trend p-value</strong>.</p>",
+            "<h3>1.4. Significance Testing</h3>",
+            "<p>Each time series for each subgroup is treated as an independent hypothesis test. This analysis intentionally avoids corrections for multiple comparisons (e.g., Bonferroni, FDR) to maximize sensitivity and ensure all potentially significant events are surfaced for review. The goal is to provide a comprehensive screen rather than a confirmatory analysis.</p>"
+            f"<p>A conventional significance level (alpha) of <strong>{p_thresh}</strong> is used. A finding is reported as statistically significant if its p-value is less than this threshold.</p>",
+            "<h3>1.5. Noise Reduction</h3>",
+            "<p>For time series with very low frequency (a historical average of less than one event per week), a single event can be flagged as a statistically significant anomaly. To improve the signal-to-noise ratio of the results, such findings are filtered out. An anomaly is only reported if the observed count is greater than 1, or if the historical average is 1 or greater.</p>"
+        ]
+
+    def _generate_appendix(self):
+        return [
+            "<hr><h2>Appendix: Definition of Terms</h2>",
+            "<ul>",
+            "<li><strong>Poisson Distribution</strong>: A discrete probability distribution for the counts of events that occur randomly in a given interval of time or space.</li>",
+            "<li><strong>Negative Binomial Distribution</strong>: A generalization of the Poisson distribution that allows for overdispersion, where the variance is greater than the mean.</li>",
+            "<li><strong>P-value</strong>: The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.</li>",
+            "<li><strong>Z-score</strong>: A measure of how many standard deviations an observation or data point is from the mean of a distribution. It provides a standardized measure of an anomaly's magnitude.</li>",
+            "</ul>",
+            "</body></html>"
+        ]
+
+# --- Main Stage Class ---
+
 class Stage3UnivariateAnomaly:
     def __init__(self, job_id: str, config: dict, results_dir: str, redis_client=None, data_sources: list = None):
         self.job_id = job_id
@@ -17,13 +437,15 @@ class Stage3UnivariateAnomaly:
         self.data_sources = data_sources
         self.job_dir = os.path.join(results_dir, self.job_id)
         os.makedirs(self.job_dir, exist_ok=True)
+        self.json_storage = JsonStorageModel()
+        self.image_storage = ImageStorageModel()
 
     @property
     def name(self) -> str:
         return "stage3_univariate_anomaly"
 
     def get_reporter(self) -> Optional[object]:
-        return Stage3Reporter()
+        return Stage3Reporter(self.job_id, self.image_storage)
 
     def generate_and_save_report(self, results: dict, df: pd.DataFrame):
         """Generates and saves the report for the stage, if a reporter is available."""
@@ -33,27 +455,9 @@ class Stage3UnivariateAnomaly:
             results['__filepath__'] = os.path.join(self.job_dir, f"{self.name}.json")
             
             report_content = reporter.generate_report(results, df)
-            report_filename = f"report_{self.name}.{reporter.file_extension}"
-            self._save_report(report_content, report_filename)
-
-    def _save_results(self, results: dict, filename: str) -> str:
-        """
-        Helper method to save a dictionary as JSON to the local results directory.
-        Returns the path to the saved file.
-        """
-        output_path = os.path.join(self.job_dir, filename)
-        
-        print(f"Saving results for job {self.job_id} to {output_path}")
-        with open(output_path, 'w') as f:
-            json.dump(results, f, indent=4)
-            
-        return output_path
-
-    def _save_report(self, content: str, filename: str):
-        """Saves the report content to a file in the job's directory."""
-        filepath = os.path.join(self.job_dir, filename)
-        with open(filepath, 'w') as f:
-            f.write(content)
+            report_filename = f"report_{self.name}.html"
+            # For simplicity, I'll use the backend from JsonStorageModel.
+            self.json_storage.backend.save_bytes(f"{self.job_id}/{report_filename}", report_content.encode('utf-8'))
 
     def _analyze_time_series(
         self, 
diff --git a/backend/stages/stage4_h3_anomaly.py b/backend/stages/stage4_h3_anomaly.py
index 7b2318b..fd984ab 100644
--- a/backend/stages/stage4_h3_anomaly.py
+++ b/backend/stages/stage4_h3_anomaly.py
@@ -5,11 +5,127 @@ import h3
 import os
 import json
 import re
-from reporting.visualizations.common import plot_comparative_time_series
+import matplotlib.pyplot as plt
 from typing import Optional, List
 import time
 import tempfile
 from itertools import groupby
+from core.storage import JsonStorageModel, ImageStorageModel
+
+# --- Visualization Functions (Consolidated) ---
+
+def plot_comparative_time_series(
+    group_series: pd.Series,
+    group_name: str,
+    city_wide_series: Optional[pd.Series],
+    primary_col: str,
+    secondary_col: str,
+    anomaly_points: Optional[list] = None
+) -> plt.Figure:
+    """Generates a plot comparing a specific group's time series to the city-wide equivalent."""
+    plt.style.use('seaborn-v0_8-whitegrid')
+    fig, ax = plt.subplots(figsize=(15, 7))
+
+    # Plot the specific group's data
+    ax.plot(group_series.index, group_series.values, 'o-', label=f'{primary_col}: {group_name}', color='blue', linewidth=2)
+
+    # If city-wide data is available, plot it on a secondary y-axis
+    if city_wide_series is not None:
+        ax2 = ax.twinx()
+        ax2.plot(city_wide_series.index, city_wide_series.values, 's--', label='City-Wide', color='gray', alpha=0.7)
+        ax.set_ylabel(f'Incident Count ({group_name})', color='blue')
+        ax2.set_ylabel('Incident Count (City-Wide)', color='gray')
+        ax.tick_params(axis='y', labelcolor='blue')
+        ax2.tick_params(axis='y', labelcolor='gray')
+        lines, labels = ax.get_legend_handles_labels()
+        lines2, labels2 = ax2.get_legend_handles_labels()
+        ax2.legend(lines + lines2, labels + labels2, loc=0)
+    else:
+        ax.set_ylabel('Incident Count')
+        ax.legend(loc=0)
+
+    # Highlight anomalous points on the primary series
+    if anomaly_points:
+        for point in anomaly_points:
+            ts = pd.to_datetime(point['week'])
+            y_value = point.get('deseasonalized_count', point.get('count'))
+            if y_value is not None:
+                ax.plot(ts, y_value, 'ro', markersize=12, alpha=0.8, label=f"Anomaly on {point['week']}")
+
+    ax.set_title(f"Comparison for '{secondary_col}': {group_name} vs. City-Wide", fontsize=16)
+    ax.set_xlabel("Date")
+    
+    plt.xticks(rotation=45)
+    plt.tight_layout()
+    return fig
+
+# --- Reporter Class (Consolidated) ---
+
+class Stage4Reporter:
+    """
+    Generates an HTML report for Stage 4 by populating a static template
+    which uses JavaScript to load and render the results dynamically.
+    """
+
+    @property
+    def file_extension(self) -> str:
+        return "html"
+
+    def generate_report(self, data: dict, df: Optional[pd.DataFrame] = None) -> str:
+        params = data.get('parameters', {})
+        h3_resolution = params.get('h3_resolution', 'N/A')
+        secondary_col_name = params.get('secondary_group_col', 'Secondary Group')
+        p_value_threshold = 0.05
+
+        # Determine the path to the template file
+        # Assumes the template is in a 'templates' subdirectory next to this file
+        # Since we moved the reporter to stages/, we need to adjust the path or assume templates are in backend/reporting/templates
+        # For now, let's assume the templates folder is still in reporting/templates
+        template_path = os.path.join(os.path.dirname(__file__), '..', 'reporting', 'templates', 'stage4_report_template.html')
+
+        try:
+            with open(template_path, 'r') as f:
+                template_str = f.read()
+        except FileNotFoundError:
+            return "<h1>Error</h1><p>Report template not found.</p>"
+
+        # Generate methodology and appendix content
+        methodology = self._generate_methodology(secondary_col_name, h3_resolution, p_value_threshold)
+        appendix = self._generate_appendix()
+
+        # Replace placeholders in the template
+        report_content = template_str.replace('{{METHODOLOGY}}', "\n".join(methodology))
+        report_content = report_content.replace('{{APPENDIX}}', "\n".join(appendix))
+
+        return report_content
+
+    def _generate_methodology(self, secondary_col, h3_res, p_thresh):
+        return [
+            "<h2>1. Methodology</h2>",
+            "The analytical approach comprises several sequential steps: spatial aggregation, data preparation, model selection, and independent significance testing.",
+            "<h3>1.1. Spatial Aggregation and Data Preparation</h3>",
+            f"<p>Raw incident data points were assigned to a hexagonal H3 cell based on their latitude and longitude coordinates, using H3 resolution <code>{h3_res}</code>. The data was then further partitioned into subgroups based on unique combinations of the H3 cell and the "
+            f"<code>{secondary_col}</code> field. For each subgroup, a time series was constructed by resampling the data into weekly incident counts. A minimum of eight weeks of data was required for a subgroup to be included in the analysis.</p>",
+            "<h3>1.2. Probabilistic Model Selection and Anomaly/Trend Detection</h3>",
+            "<p>The methodology for model selection (Poisson vs. Negative Binomial), anomaly detection (p-value and z-score for the last four weeks), and trend detection (linear regression on the last four weeks) is identical to that used in the standard univariate analysis. Please refer to that report for detailed descriptions.</p>",
+            "<h3>1.3. Significance Testing and Noise Reduction</h3>",
+            f"<p>Each time series is treated as an independent hypothesis test. A conventional significance level (alpha) of <strong>{p_thresh}</strong> is used. A finding is reported as statistically significant if its p-value is less than this threshold. To reduce noise, anomalies are only reported for low-frequency series if the observed count is greater than 1.</p>"
+        ]
+
+    def _generate_appendix(self):
+        return [
+            "<hr><h2>Appendix: Definition of Terms</h2>",
+            "<ul>",
+            "<li><strong>H3</strong>: A geospatial indexing system that partitions the world into hexagonal cells. It allows for efficient spatial analysis at various resolutions.</li>",
+            "<li><strong>Leaflet.js</strong>: An open-source JavaScript library for mobile-friendly interactive maps.</li>",
+            "<li><strong>Poisson Distribution</strong>: A discrete probability distribution for the counts of events that occur randomly in a given interval of time or space.</li>",
+            "<li><strong>Negative Binomial Distribution</strong>: A generalization of the Poisson distribution that allows for overdispersion, where the variance is greater than the mean.</li>",
+            "<li><strong>P-value</strong>: The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.</li>",
+            "<li><strong>Z-score</strong>: A measure of how many standard deviations an observation or data point is from the mean of a distribution. It provides a standardized measure of an anomaly's magnitude.</li>",
+            "</ul>"
+        ]
+
+# --- Main Stage Class ---
 
 class Stage4H3Anomaly:
     def __init__(self, job_id: str, config: dict, results_dir: str, redis_client=None, data_sources: list = None):
@@ -19,6 +135,8 @@ class Stage4H3Anomaly:
         self.data_sources = data_sources
         self.job_dir = os.path.join(results_dir, self.job_id)
         os.makedirs(self.job_dir, exist_ok=True)
+        self.json_storage = JsonStorageModel()
+        self.image_storage = ImageStorageModel()
 
     @property
     def name(self) -> str:
@@ -27,22 +145,17 @@ class Stage4H3Anomaly:
     def get_reporter(self) -> Optional[object]:
         """
         Stage 4 uses a dynamic, client-side viewer instead of a static report.
-        Therefore, it does not have a Python-based reporter.
+        However, we provide the reporter class here for consistency.
         """
-        return None
+        return Stage4Reporter()
 
     def _save_results(self, results: dict, filename: str) -> str:
         """
         Helper method to save a dictionary as JSON to the local results directory.
         Returns the path to the saved file.
         """
-        output_path = os.path.join(self.job_dir, filename)
-        
-        print(f"Saving results for job {self.job_id} to {output_path}")
-        with open(output_path, 'w') as f:
-            json.dump(results, f, indent=4)
-            
-        return output_path
+        print(f"Saving results for job {self.job_id} to storage: {filename}")
+        return self.json_storage.save(self.job_id, filename, results)
 
     def _update_progress(self, progress: int, stage_detail: str):
         """Updates the job progress in Redis if a client is available."""
@@ -199,13 +312,10 @@ class Stage4H3Anomaly:
         """
         # Check if stage should be skipped
         skip_existing = self.config.get('skip_existing', False)
-        output_path = os.path.join(self.job_dir, f"{self.name}.json")
-        if skip_existing and os.path.exists(output_path):
+        filename = f"{self.name}.json"
+        if skip_existing and self.json_storage.exists(self.job_id, filename):
             print(f"Skipping stage {self.name} as output already exists.")
-            with open(output_path, 'r') as f:
-                # When skipping, we must return the full structure, so we load it.
-                # This is acceptable as it's not part of the main processing path.
-                return json.load(f)
+            return self.json_storage.load(self.job_id, filename)
 
         if not self.data_sources:
             raise ValueError("data_sources list must be provided to Stage4H3Anomaly constructor for multi-file processing.")
@@ -238,6 +348,7 @@ class Stage4H3Anomaly:
         })
 
         # --- Create a temporary file for disk-based aggregation ---
+        # We keep this local because it's an intermediate processing step, not a final result
         temp_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.csv', dir=self.job_dir)
         temp_filename = temp_file.name
         # Write header for the temp file
@@ -327,8 +438,6 @@ class Stage4H3Anomaly:
             # --- PASS 2: ANALYZE FROM DISK ---
             self._update_progress(50, "Aggregating unique groups and analyzing")
             
-            # First, sum up counts for the same group/week from different chunks
-            # This is the most memory-intensive part of the new approach, but still far less than before.
             agg_df = pd.read_csv(temp_filename)
             final_counts_df = agg_df.groupby(['h3_index', 'secondary_group', 'week'])['count'].sum().reset_index()
             final_counts_df['week'] = pd.to_datetime(final_counts_df['week'])
@@ -336,79 +445,65 @@ class Stage4H3Anomaly:
             localized_groups = final_counts_df[final_counts_df['h3_index'] != 'city-wide'].groupby(['h3_index', 'secondary_group'])
             city_wide_groups = final_counts_df[final_counts_df['h3_index'] == 'city-wide'].groupby('secondary_group')
             
+            # Instead of streaming to a file, we build the result object in memory to pass to JsonStorageModel
+            # This supports the abstraction requirement (e.g. saving to DB later)
+            final_results_list = []
+            city_wide_results_list = []
+            localized_results_for_plotting = []
+
             total_groups = len(localized_groups) + len(city_wide_groups)
             processed_groups = 0
             last_update_time = time.time()
 
-            os.makedirs(self.job_dir, exist_ok=True)
-            with open(output_path, 'w') as f:
-                f.write(json.dumps({"status": "success", "stage_name": self.name, "parameters": stage_params})[:-1])
-                f.write(', "results": [')
-
-                self._update_progress(55, f"Analyzing {len(localized_groups)} localized groups")
-                is_first_result = True
-                localized_results_for_plotting = []
-
-                for (h3_index, group2), group_df in localized_groups:
-                    analysis_result = self._analyze_time_series(
-                        group_df.rename(columns={'week': '__timestamp'}), '__timestamp', end_date, 
-                        analysis_weeks_trend, analysis_weeks_anomaly, min_trend_events, p_value_trend
-                    )
-                    
-                    if analysis_result:
-                        analysis_result[f"h3_index_{h3_resolution}"] = h3_index
-                        # Ensure secondary group is a native Python type
-                        analysis_result['secondary_group'] = group2.item() if isinstance(group2, np.generic) else group2
-                        lat, lon = h3.cell_to_latlng(h3_index)
-                        analysis_result['lat'] = lat
-                        analysis_result['lon'] = lon
-                        
-                        result_to_save = analysis_result.copy()
-                        if not save_full_series:
-                            del result_to_save['full_weekly_series']
-
-                        if not is_first_result: f.write(',')
-                        json.dump(result_to_save, f)
-                        is_first_result = False
-                        localized_results_for_plotting.append(analysis_result)
-
-                    processed_groups += 1
-                    current_time = time.time()
-                    if current_time - last_update_time > 2 or processed_groups % 100 == 0:
-                        progress = 55 + int(40 * (processed_groups / total_groups))
-                        self._update_progress(progress, f"Analysis: Processed {processed_groups}/{total_groups} groups")
-                        last_update_time = current_time
+            self._update_progress(55, f"Analyzing {len(localized_groups)} localized groups")
+            for (h3_index, group2), group_df in localized_groups:
+                analysis_result = self._analyze_time_series(
+                    group_df.rename(columns={'week': '__timestamp'}), '__timestamp', end_date, 
+                    analysis_weeks_trend, analysis_weeks_anomaly, min_trend_events, p_value_trend
+                )
                 
-                f.write('], "city_wide_results": [')
-
-                self._update_progress(95, f"Analyzing {len(city_wide_groups)} city-wide groups")
-                is_first_result = True
-                city_wide_results = []
-                for group2, group_df in city_wide_groups:
-                    analysis_result = self._analyze_time_series(
-                        group_df.rename(columns={'week': '__timestamp'}), '__timestamp', end_date, 
-                        analysis_weeks_trend, analysis_weeks_anomaly, min_trend_events, p_value_trend
-                    )
-                    if analysis_result:
-                        # Ensure secondary group is a native Python type
-                        analysis_result['secondary_group'] = group2.item() if isinstance(group2, np.generic) else group2
-                        analysis_result['primary_group_name'] = "City-Wide"
-                        city_wide_results.append(analysis_result)
-                        
-                        result_to_save = analysis_result.copy()
-                        if not save_full_series:
-                            del result_to_save['full_weekly_series']
-
-                        if not is_first_result: f.write(',')
-                        json.dump(result_to_save, f)
-                        is_first_result = False
-
-                f.write(']}')
+                if analysis_result:
+                    analysis_result[f"h3_index_{h3_resolution}"] = h3_index
+                    # Ensure secondary group is a native Python type
+                    analysis_result['secondary_group'] = group2.item() if isinstance(group2, np.generic) else group2
+                    lat, lon = h3.cell_to_latlng(h3_index)
+                    analysis_result['lat'] = lat
+                    analysis_result['lon'] = lon
+                    
+                    result_to_save = analysis_result.copy()
+                    if not save_full_series:
+                        del result_to_save['full_weekly_series']
+
+                    final_results_list.append(result_to_save)
+                    localized_results_for_plotting.append(analysis_result)
+
+                processed_groups += 1
+                current_time = time.time()
+                if current_time - last_update_time > 2 or processed_groups % 100 == 0:
+                    progress = 55 + int(40 * (processed_groups / total_groups))
+                    self._update_progress(progress, f"Analysis: Processed {processed_groups}/{total_groups} groups")
+                    last_update_time = current_time
+            
+            self._update_progress(95, f"Analyzing {len(city_wide_groups)} city-wide groups")
+            for group2, group_df in city_wide_groups:
+                analysis_result = self._analyze_time_series(
+                    group_df.rename(columns={'week': '__timestamp'}), '__timestamp', end_date, 
+                    analysis_weeks_trend, analysis_weeks_anomaly, min_trend_events, p_value_trend
+                )
+                if analysis_result:
+                    # Ensure secondary group is a native Python type
+                    analysis_result['secondary_group'] = group2.item() if isinstance(group2, np.generic) else group2
+                    analysis_result['primary_group_name'] = "City-Wide"
+                    city_wide_results_list.append(analysis_result)
+                    
+                    result_to_save = analysis_result.copy()
+                    if not save_full_series:
+                        del result_to_save['full_weekly_series']
 
             # --- Generate plots for significant findings ---
             if plot_generation != 'none':
                 self._update_progress(98, "Generating plots for significant findings")
-                city_wide_map = {item['secondary_group']: item for item in city_wide_results}
+                city_wide_map = {item['secondary_group']: item for item in city_wide_results_list}
                 
                 generated_plots = set()
                 h3_col = f"h3_index_{h3_resolution}"
@@ -448,26 +543,28 @@ class Stage4H3Anomaly:
                         sanitized_sec_group = self._sanitize_filename(str(sec_group))
                         plot_filename = f"plot_{h3_index}_{sanitized_sec_group}.png"
 
-                        plot_comparative_time_series(
+                        fig = plot_comparative_time_series(
                             group_series=group_series,
                             group_name=h3_index,
                             city_wide_series=city_wide_series,
                             primary_col="H3 Cell",
                             secondary_col=sec_group,
-                            output_dir=self.job_dir,
-                            anomaly_points=significant_anomalies,
-                            filename_override=plot_filename
+                            anomaly_points=significant_anomalies
                         )
+                        self.image_storage.save_plot(self.job_id, plot_filename, fig)
                         generated_plots.add(plot_key)
 
             self._update_progress(100, "Finalizing results")
             
-            return {
+            final_output = {
                 "status": "success",
                 "stage_name": self.name,
                 "parameters": stage_params,
-                "results_summary": f"{len(localized_results_for_plotting)} localized results and {len(city_wide_results)} city-wide results written to {output_path}"
+                "results": final_results_list,
+                "city_wide_results": city_wide_results_list
             }
+            self._save_results(final_output, f"{self.name}.json")
+            return final_output
 
         finally:
             # --- Cleanup ---
diff --git a/docker-compose.dev.yml b/docker-compose.dev.yml
index b9861d4..1d16dbf 100644
--- a/docker-compose.dev.yml
+++ b/docker-compose.dev.yml
@@ -3,6 +3,9 @@ services:
     image: "redis:alpine"
     ports:
       - "6379:6379"
+    environment:
+      - REDIS_PASSWORD=${REDIS_PASSWORD}
+    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD}"]
     
 
   backend:
@@ -15,8 +18,10 @@ services:
       - ./storage:/app/storage
     ports:
       - "8030:8080"
+    env_file:
+      - .env
     environment:
-      - REDIS_URL=redis://redis:6379/0
+      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
     depends_on:
       - redis
     command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080", "--reload"]
@@ -30,8 +35,10 @@ services:
     volumes:
       - ./backend:/app
       - ./storage:/app/storage
+    env_file:
+      - .env
     environment:
-      - REDIS_URL=redis://redis:6379/0
+      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
     depends_on:
       - redis
     command: ["celery", "-A", "app.worker:app", "worker", "--loglevel=info","--concurrency=1"]
diff --git a/docker-compose.yml b/docker-compose.yml
index 1e531b9..7a57517 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -7,7 +7,13 @@ services:
     expose:
       - "8000"
     environment:
-      - YOUR_ENV_VARIABLES_HERE
+      - STORAGE_TYPE=${STORAGE_TYPE}
+      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
+      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
+      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
+      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
+      - DOMAIN=${DOMAIN}
+      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL}
     networks:
       - app-network
     init: true
diff --git a/frontend/README.md b/frontend/README.md
index 3765e0b..93999ef 100644
--- a/frontend/README.md
+++ b/frontend/README.md
@@ -47,3 +47,133 @@ npm run build
 ```
 
 This command bundles the application into static files and places them in the `frontend/build` directory. The Python backend is configured to serve these files when running in a production environment.
+
+## Distributed Development (Remote Worker)
+
+It is possible to run the `backend` and `redis` services on a remote server while running the Celery `worker` on your local machine. This is useful for offloading heavy processing tasks from the server.
+
+**Prerequisites:**
+- A remote server with Docker and a public domain name (e.g., `lotusfiremeditation.org`).
+- Your local machine with the project code and Docker installed.
+- S3 storage is **required**, as both the remote backend and local worker need access to the same storage artifacts.
+
+### On the Remote Server
+
+1.  **Configure `.env` file**:
+    -   Create or edit the `.env` file in the project root.
+    -   Set a strong `REDIS_PASSWORD`.
+    -   Set `STORAGE_TYPE=s3` and provide your S3 bucket details and AWS credentials.
+    -   Set `CELERY_BROKER_URL` to your public Redis URL, including the password (e.g., `redis://:your-password@lotusfiremeditation.org:6379/0`).
+    -   Set `INTERNAL_API_HOSTNAME` to your public backend URL (e.g., `http://lotusfiremeditation.org:8080`).
+
+2.  **Launch Services**:
+    -   Use the `docker-compose.server.yml` file to start the `redis` and `backend` containers.
+
+    ```bash
+    sudo docker-compose -f docker-compose.server.yml up --build -d
+    ```
+
+### On Your Local Machine
+
+This setup now involves two potential types of workers: one for data analysis and one for AI completions.
+
+1.  **Configure `.env` file**:
+    -   Create or edit the `.env` file in your local project root.
+    -   Set the `REDIS_PASSWORD` to match the server.
+    -   Set `STORAGE_TYPE=s3` with the same S3/AWS configuration as the server.
+    -   Set `REDIS_URL` to your public Redis URL, including the password.
+    -   Set `INTERNAL_API_HOSTNAME` to your public backend URL.
+    -   Set `OLLAMA_URL` to point to your local Ollama service (e.g., `http://host.docker.internal:11434`).
+
+2.  **Launch the Analysis Worker**:
+    -   To process standard data analysis jobs, use the `docker-compose.analysis-worker.yml` file.
+    ```bash
+    docker-compose -f docker-compose.analysis-worker.yml up --build
+    ```
+
+3.  **Launch the Completions Worker**:
+    -   To process AI completion jobs from the Laravel app, use the `docker-compose.completions-worker.yml` file. Ensure your local Ollama service is running.
+    ```bash
+    docker-compose -f docker-compose.completions-worker.yml up --build
+    ```
+
+The workers will connect to the Redis instance on your server and start processing tasks from their respective queues.
+
+### Testing the Distributed Setup Locally
+
+You can simulate the server/worker split on your local machine to test the distributed configuration. This involves running the server components and the worker component in separate terminals.
+
+1.  **Configure `.env` file for Local Testing**:
+    -   Edit the `.env` file in your project root.
+    -   Set a `REDIS_PASSWORD`.
+    -   Set `STORAGE_TYPE=s3` and provide your S3 credentials.
+    -   Point the URLs to `host.docker.internal`. This allows standard containers (like the analysis worker) to reach services on your host. The completions worker, which uses host networking, will override these values internally.
+        ```properties
+        REDIS_PASSWORD=your-secure-password
+        REDIS_URL=redis://:your-secure-password@host.docker.internal:6379/0
+        CELERY_BROKER_URL=redis://:your-secure-password@host.docker.internal:6379/0
+        INTERNAL_API_HOSTNAME=http://host.docker.internal:8030
+        OLLAMA_URL=http://host.docker.internal:11434
+        ```
+
+2.  **Launch Server Components**:
+    -   In your **first terminal**, run `docker-compose.server.yml`. This will start the `redis` and `backend` services and make them available on your host machine's ports.
+    ```bash
+    docker-compose -f docker-compose.server.yml up --build
+    ```
+
+3.  **Launch the Workers**:
+    -   In a **second terminal**, run `docker-compose.analysis-worker.yml` to start the worker for analysis jobs.
+    ```bash
+    docker-compose -f docker-compose.analysis-worker.yml up --build
+    ```
+    -   In a **third terminal**, run `docker-compose.completions-worker.yml` to start the worker for AI completion jobs.
+    ```bash
+    docker-compose -f docker-compose.completions-worker.yml up --build
+    ```
+
+You can now submit an analysis job to `http://localhost:8030/api/v1/jobs` and see the analysis worker process it. You can submit a completion job to `http://localhost:8030/api/v1/completions` and see the completions worker process it. Both job types will produce artifacts retrievable via the `/api/v1/jobs/{job_id}/results` endpoint.
+
+### Testing the Completions Endpoint with `curl`
+
+You can quickly test the AI completions workflow from your terminal using `curl`. Make sure your server and completions worker containers are running.
+
+1.  **Submit a Completion Job**:
+    Send a POST request with a unique `job_id` and a `prompt`.
+
+    ```bash
+    curl -X POST http://localhost:8030/api/v1/completions \
+    -H "Content-Type: application/json" \
+    -d '{
+      "model": "llama3:8b",
+      "job_id": "my-curl-test-01",
+      "prompt": "Why is the sky blue? Explain it simply."
+    }'
+    ```
+    You will get a response with URLs to check the job's status and results.
+
+2.  **Check the Job Status**:
+    Use the `status_url` from the previous response to poll the job's status.
+
+    ```bash
+    # Replace the job_id with your own
+    curl http://localhost:8030/api/v1/jobs/my-curl-test-01/status
+    ```
+    Initially, this will show `{"status":"queued"}`. After the worker processes it, it will show `{"status":"completed", ...}`.
+
+3.  **Retrieve the Result**:
+    Once the job is complete, you can list the available result files.
+
+    ```bash
+    # Replace the job_id with your own
+    curl http://localhost:8030/api/v1/jobs/my-curl-test-01/results
+    ```
+    This will return a JSON object with a link to `completion.json`.
+
+4.  **View the Final JSON Output**:
+    Use the URL for `completion.json` to see the response from Ollama.
+
+    ```bash
+    # Replace the job_id with your own
+    curl http://localhost:8030/api/v1/jobs/my-curl-test-01/results/completion.json
+    ```
